Di seguito trovi il codice sorgente e la configurazione del progetto.
Usa questi file come contesto per le risposte.
==================================================


==================== FILE: src/metrics/metrics_service.py ====================
from flask import Flask, jsonify, request
from pymongo import MongoClient
from datetime import datetime, timedelta
import os, time, threading

app = Flask(__name__)

POD_NAME = os.getenv("POD_NAME", "unknown-pod")
MONGO_URI = os.environ["MONGO_URI"]

# Configurazione Mongo con Timeout breve
client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=3000)
db = client.student_events
collection = db.events

_cache = {}
_cache_lock = threading.Lock()
CACHE_TTL = int(os.getenv("METRICS_CACHE_TTL", 10))

def cache_get(key):
    with _cache_lock:
        entry = _cache.get(key)
        if not entry: return None
        value, ts = entry
        if time.time() - ts > CACHE_TTL:
            del _cache[key]
            return None
        return value

def cache_set(key, value):
    with _cache_lock:
        _cache[key] = (value, time.time())

@app.route("/healthz")
def healthz():
    mongo_ok = False
    try:
        client.admin.command('ping')
        mongo_ok = True
    except Exception as e:
        print(f"[HEALTH FAIL] Mongo unreachable: {e}", flush=True)

    status = {
        "status": "ok" if mongo_ok else "degraded", 
        "mongo_connected": mongo_ok, 
        "processed_by": POD_NAME
    }
    return jsonify(status), 200 if mongo_ok else 503


# 1. Totale logins (Cached)
@app.route("/metrics/logins", methods=["GET"])
def total_logins():
    key = "total_logins"
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    count = collection.count_documents({"type": "login"})
    resp = {"total_logins": count}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 2. Avg logins per user (Cached)
@app.route("/metrics/logins/average", methods=["GET"])
def avg_logins_per_user():
    key = "avg_logins"
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "login"}},
        {"$group": {"_id": "$user_id", "count": {"$sum": 1}}},
        {"$group": {"_id": None, "average_logins": {"$avg": "$count"}}}
    ]
    result = list(collection.aggregate(pipeline))
    resp = result[0] if result else {"average_logins": 0}
    if "_id" in resp: del resp["_id"]
    
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 3. Downloads (OTTIMIZZATO con $facet)
@app.route("/metrics/downloads", methods=["GET"])
def downloads():
    page = int(request.args.get("page", "1"))
    per_page = int(request.args.get("per_page", "50"))
    skip = (page - 1) * per_page
    key = f"downloads_p{page}_pp{per_page}"
    
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "download_materiale"}},
        {"$group": {"_id": "$materiale_id", "downloads": {"$sum": 1}}},
        {"$sort": {"downloads": -1}},
        {"$facet": {
            "metadata": [{"$count": "total"}],
            "data": [{"$skip": skip}, {"$limit": per_page}]
        }}
    ]
    
    result = list(collection.aggregate(pipeline))[0]
    total = result["metadata"][0]["total"] if result["metadata"] else 0
    items = [{"materiale_id": r["_id"], "downloads": r["downloads"]} for r in result["data"]]
    
    resp = {"data": items, "page": page, "per_page": per_page, "total": total}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 4. Exams (OTTIMIZZATO con $facet)
@app.route("/metrics/exams", methods=["GET"])
def exams():
    page = int(request.args.get("page", "1"))
    per_page = int(request.args.get("per_page", "50"))
    skip = (page - 1) * per_page
    key = f"exams_p{page}_pp{per_page}"
    
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "prenotazione_esame"}},
        {"$group": {"_id": "$course_id", "prenotazioni": {"$sum": 1}}},
        {"$sort": {"prenotazioni": -1}},
        {"$facet": {
            "metadata": [{"$count": "total"}],
            "data": [{"$skip": skip}, {"$limit": per_page}]
        }}
    ]
    
    result = list(collection.aggregate(pipeline))[0]
    total = result["metadata"][0]["total"] if result["metadata"] else 0
    items = [{"course_id": r["_id"], "prenotazioni": r["prenotazioni"]} for r in result["data"]]
    
    resp = {"data": items, "page": page, "per_page": per_page, "total": total}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 5. Quiz Success Rate
@app.route("/metrics/quiz/success-rate", methods=["GET"])
def quiz_success_rate():
    key = "quiz_success_rate"
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "quiz_submission"}},
        {"$group": {
            "_id": None,
            "total": {"$sum": 1},
            "success": {"$sum": {"$cond": [{"$gte": ["$score", 18]}, 1, 0]}}
        }},
        {"$project": {"_id": 0, "success_rate": {"$cond": [{"$eq": ["$total", 0]}, 0, {"$multiply": [{"$divide": ["$success", "$total"]}, 100]}]}}}
    ]
    result = list(collection.aggregate(pipeline))
    resp = result[0] if result else {"success_rate": 0}
    
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 6. Activity Trend (LIVE - Usa _ingest_ts)
@app.route("/metrics/activity/last7days", methods=["GET"])
def activity_trend():
    # Usa _ingest_ts aggiunto dal Consumer FIXATO
    since = datetime.utcnow() - timedelta(days=7)
    pipeline = [
        {"$match": {"_ingest_ts": {"$gte": since}}},
        {"$group": {"_id": {"$dateToString": {"format": "%Y-%m-%d", "date": "$_ingest_ts"}}, "count": {"$sum": 1}}},
        {"$sort": {"_id": 1}}
    ]
    result = list(collection.aggregate(pipeline))
    return jsonify({"data": result, "source": "live", "processed_by": POD_NAME})

# 7. Avg Score Course (OTTIMIZZATO con $facet)
@app.route("/metrics/quiz/average-score", methods=["GET"])
def avg_score_per_course():
    page = int(request.args.get("page", "1"))
    per_page = int(request.args.get("per_page", "50"))
    skip = (page - 1) * per_page
    key = f"avg_score_p{page}_pp{per_page}"
    
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "quiz_submission"}},
        {"$group": {"_id": "$course_id", "average_score": {"$avg": "$score"}}},
        {"$sort": {"average_score": -1}},
        {"$facet": {
            "metadata": [{"$count": "total"}],
            "data": [{"$skip": skip}, {"$limit": per_page}]
        }}
    ]
    
    result = list(collection.aggregate(pipeline))[0]
    total = result["metadata"][0]["total"] if result["metadata"] else 0
    items = [{"course_id": r["_id"], "average_score": r["average_score"]} for r in result["data"]]
    
    resp = {"data": items, "page": page, "per_page": per_page, "total": total}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)
==================== FINE FILE: src/metrics/metrics_service.py ====================

==================== FILE: src/metrics/Dockerfile ====================
FROM python:3.11-slim

WORKDIR /app

COPY metrics_service.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "metrics_service.py"]

==================== FINE FILE: src/metrics/Dockerfile ====================

==================== FILE: src/metrics/requirements.txt ====================
flask
pymongo
==================== FINE FILE: src/metrics/requirements.txt ====================

==================== FILE: src/producer/Dockerfile ====================
FROM python:3.11-slim

WORKDIR /app
COPY producer.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 5000

CMD ["python", "producer.py"]

==================== FINE FILE: src/producer/Dockerfile ====================

==================== FILE: src/producer/requirements.txt ====================
kafka-python
flask
==================== FINE FILE: src/producer/requirements.txt ====================

==================== FILE: src/producer/producer.py ====================
from flask import Flask, request, jsonify
from kafka import KafkaProducer
from kafka.errors import NoBrokersAvailable, KafkaError
import json, os, uuid, time, threading, queue

app = Flask(__name__)

# --- Configurazione (Variabili d'ambiente e default) ---
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
# Percorso del certificato CA per la connessione sicura a Kafka
KAFKA_CA = os.getenv("KAFKA_CA", "/etc/ssl/certs/kafka/ca.crt")
# Nome del pod corrente, utile per il debug e per vedere il load balancing
POD_NAME = os.getenv("POD_NAME", "unknown-pod")
TOPIC = os.getenv("KAFKA_TOPIC", "student-events")

# Configurazione della coda locale (Fallback)
# Se Kafka è giù, accumula fino a 500 messaggi in RAM prima di rifiutare (Backpressure)
MAX_LOCAL_QUEUE = int(os.getenv("MAX_LOCAL_QUEUE", 500))   # max events to hold locally
# Intervallo in secondi per il thread che prova a svuotare la coda locale verso Kafka
QUEUE_FLUSH_INTERVAL = float(os.getenv("QUEUE_FLUSH_INTERVAL", 3.0))  # 3 secondi

# Coda thread-safe per il buffering locale
local_queue = queue.Queue(maxsize=MAX_LOCAL_QUEUE)
producer = None
# Lock per garantire l'accesso thread-safe all'oggetto producer
producer_lock = threading.Lock()
shutdown_flag = threading.Event()

# Contatori per le metriche interne (monitoraggio stato)
sent_counter = 0
queued_counter = 0
failed_sends = 0
counters_lock = threading.Lock()

# --- Inizializzazione Producer (Tentativo non bloccante) ---
def init_producer():
    global producer
    try:
        # Configurazione del client Kafka con SASL/SCRAM e SSL
        p = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP,
            security_protocol="SASL_SSL" if SASL_USERNAME else "PLAINTEXT",
            sasl_mechanism="SCRAM-SHA-512" if SASL_USERNAME else None,
            sasl_plain_username=SASL_USERNAME if SASL_USERNAME else None,
            sasl_plain_password=SASL_PASSWORD if SASL_USERNAME else None,
            ssl_cafile=KAFKA_CA if SASL_USERNAME else None,
            value_serializer=lambda v: json.dumps(v).encode("utf-8"),
            acks='all',          # Richiede conferma da tutte le repliche (Durabilità)
            retries=5,           # Riprova in caso di errori di rete temporanei
            linger_ms=10,        # Attende 10ms per raggruppare i messaggi (Batching)
            batch_size=16384,
            max_block_ms=1000    # Blocca al massimo 1s se il buffer locale di Kafka è pieno
        )
        with producer_lock:
            producer = p
        print(f"[PRODUCER {POD_NAME}] KafkaProducer initialized.", flush=True)
    except Exception as e:
        with producer_lock:
            producer = None
        print(f"[PRODUCER {POD_NAME}] Kafka init failed: {e}", flush=True)

# Tentativo di connessione all'avvio dell'applicazione
init_producer()

# --- Callback Asincrone ---
def on_send_success(record_metadata):
    global sent_counter
    with counters_lock:
        sent_counter += 1

def on_send_error(ex):
    global failed_sends
    with counters_lock:
        failed_sends += 1
    print(f"[PRODUCER {POD_NAME}] Kafka send error: {ex}", flush=True)

# # --- Thread di Background (Gestione Coda Locale) ---
# def flush_loop():
#     """
#     Thread che gira in background per svuotare la 'local_queue'.
#     Gestisce il reinvio dei messaggi accumulati quando Kafka era irraggiungibile.
#     """
#     global producer, queued_counter
#     while not shutdown_flag.is_set():
#         # Assicura che il producer sia inizializzato
#         with producer_lock:
#             if producer is None:
#                 init_producer()
        
#         # Prova a svuotare la coda messaggio per messaggio
#         while local_queue:
#             event = local_queue[0]  # Legge il primo elemento senza rimuoverlo (Peek)
#             try:
#                 with producer_lock:
#                     if producer is None:
#                         raise NoBrokersAvailable("producer non pronto")
#                     # Invio asincrono
#                     future = producer.send(TOPIC, value=event)
#                     future.add_callback(on_send_success)
#                     future.add_errback(on_send_error)
                
#                 # Se send() non ha dato errori immediati, rimuovo dalla coda locale
#                 local_queue.popleft()
#                 queued_counter = max(0, queued_counter - 1)
#             except NoBrokersAvailable:
#                 # Kafka ancora giù, interrompo il ciclo e riprovo dopo lo sleep
#                 break
#             except KafkaError as e:
#                 # Errore specifico di Kafka, loggho e riprovo
#                 print(f"[PRODUCER {POD_NAME}] Errore Kafka durante flush coda: {e}", flush=True)
#                 time.sleep(0.1)
        
#         # Forza l'invio dei messaggi nel buffer del client Kafka
#         try:
#             with producer_lock:
#                 if producer:
#                     producer.flush(timeout=0.1)
#         except Exception:
#             pass

#         # Attesa prima del prossimo ciclo di controllo
#         time.sleep(QUEUE_FLUSH_INTERVAL)

# --- Thread di Background (Gestione Coda Locale) ---
def flush_loop():
    global producer, queued_counter, failed_sends
    while not shutdown_flag.is_set():
        # Assicura che il producer sia inizializzato
        with producer_lock:
            p = producer
        if p is None:
            init_producer()
            with producer_lock:
                p = producer

        # Prova a svuotare la coda messaggio per messaggio
        while True:
            try:
                event = local_queue.get_nowait()
            except queue.Empty:
                break

            try:
                with producer_lock:
                    if producer is None:
                        raise NoBrokersAvailable("producer not ready")
                    # Invio asincrono
                    future = producer.send(TOPIC, value=event)
                    future.add_callback(on_send_success)
                    future.add_errback(on_send_error)
                
                with counters_lock:
                    queued_counter = max(0, queued_counter - 1)
                
                local_queue.task_done()
            except NoBrokersAvailable:
                # Kafka ancora giù, interrompo il ciclo e riprovo dopo lo sleep
                try:
                    local_queue.put_nowait(event)
                except queue.Full:
                    # Impossibile rimettere in coda, perdo l'evento
                    with counters_lock:
                        failed_sends += 1
                    print(f"[PRODUCER {POD_NAME}] CRITICAL: Lost event during flush, queue full while requeueing", flush=True)
                break
            except KafkaError as e:
                print(f"[PRODUCER {POD_NAME}] KafkaError while flushing queue: {e}", flush=True)
                time.sleep(0.1)

        # Forza l'invio dei messaggi nel buffer del client Kafka
        try:
            with producer_lock:
                if producer:
                    producer.flush(timeout=0.1)
        except Exception:
            pass

        time.sleep(QUEUE_FLUSH_INTERVAL)

# Avvio del thread di flush
flush_thread = threading.Thread(target=flush_loop, daemon=True)
flush_thread.start()

# --- Funzione Core di Invio (Logica Ibrida) ---
def try_send_event(event):
    """
    Try to send via KafkaProducer asynchronously.
    If Kafka is not available, enqueue locally (bounded queue).
    Returns tuple (status, message).
    """
    global producer, queued_counter, failed_sends

    with producer_lock:
        p = producer

    # 1. Prova invio diretto a Kafka
    if p:
        try:
            future = p.send(TOPIC, value=event)
            future.add_callback(on_send_success)
            future.add_errback(on_send_error)
            return ("sent", "event queued to kafka client buffer")
        except (NoBrokersAvailable, KafkaError) as e:
            print(f"[PRODUCER {POD_NAME}] Send error, will enqueue locally: {e}", flush=True)

    # 2. FALLBACK: Coda Locale in RAM
    try:
        local_queue.put_nowait(event)
        with counters_lock:
            global queued_counter
            queued_counter += 1
        return ("queued", f"event enqueued locally ({local_queue.qsize()}/{MAX_LOCAL_QUEUE})")
    except queue.Full:
        with counters_lock:
            global failed_sends
            failed_sends += 1
        # 3. BACKPRESSURE: Coda piena, rifiuto la richiesta
        return ("rejected", "Backpressure active: local queue full, event rejected")

# --- Endpoints ---
@app.route("/healthz")
def healthz():
    """
    Endpoint per le Probes di Kubernetes.
    Ritorna 503 se Kafka non è connesso, così il pod viene rimosso dal Load Balancer.
    """
    with producer_lock:
        p = producer
    with counters_lock:
        sc = sent_counter
        qc = queued_counter
        fs = failed_sends

    status = {
        "status": "ok" if p else "degraded",
        "producer_connected": bool(p),
        "local_queue_len": local_queue.qsize(),
        "queue_capacity": MAX_LOCAL_QUEUE,
        "sent_counter": sc,
        "queued_counter": qc,
        "failed_sends": fs,
        "processed_by": POD_NAME
    }
    return jsonify(status), 200 if p else 503

@app.route("/event/login", methods=["POST"])
def produce_login():
    data = request.json or {}
    required = ["user_id"]
    if not all(k in data for k in required):
        return jsonify({"error": "Missing required field: user_id", "processed_by": POD_NAME}), 400

    eid = data.get("event_id") or str(uuid.uuid4())
    event = {
        "event_id": eid,
        "type": "login",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        # Qui scatta il Backpressure (503 Service Unavailable)
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503

@app.route("/event/quiz", methods=["POST"])
def produce_quiz():
    data = request.json or {}
    required = ["user_id", "quiz_id", "course_id", "score"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing required fields: {required}", "processed_by": POD_NAME}), 400

    event = {
        "event_id": str(uuid.uuid4()),
        "type": "quiz_submission",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"],
        "quiz_id": data["quiz_id"],
        "course_id": data["course_id"],
        "score": data["score"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503
    
@app.route("/event/download", methods=["POST"])
def produce_download():
    data = request.json or {}
    required = ["user_id", "materiale_id", "course_id"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing required fields: {required}", "processed_by": POD_NAME}), 400

    event = {
        "event_id": str(uuid.uuid4()),
        "type": "download_materiale",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"],
        "materiale_id": data["materiale_id"],
        "course_id": data["course_id"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503

@app.route("/event/exam", methods=["POST"])
def produce_exam():
    data = request.json or {}
    required = ["user_id", "esame_id", "course_id"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing required fields: {required}", "processed_by": POD_NAME}), 400

    event = {
        "event_id": str(uuid.uuid4()),
        "type": "prenotazione_esame",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"],
        "esame_id": data["esame_id"],
        "course_id": data["course_id"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503

@app.route("/metrics", methods=["GET"])
def metrics():
    with counters_lock:
        sc = sent_counter
        qc = queued_counter
        fs = failed_sends
    return jsonify({
        "sent_counter": sc,
        "queued_local": local_queue.qsize(),
        "queue_capacity": MAX_LOCAL_QUEUE,
        "queued_counter": qc,
        "failed_sends": fs,
        "processed_by": POD_NAME
    }), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
==================== FINE FILE: src/producer/producer.py ====================

==================== FILE: src/consumer/Dockerfile ====================
FROM python:3.11-slim

WORKDIR /app
COPY consumer.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "consumer.py"]

==================== FINE FILE: src/consumer/Dockerfile ====================

==================== FILE: src/consumer/consumer.py ====================
# consumer.py (bulk + idempotency + flush timeout + heartbeat + _ingest_ts)
from kafka import KafkaConsumer
from kafka.errors import NoBrokersAvailable
from pymongo import MongoClient, errors
from pymongo.errors import ServerSelectionTimeoutError
from datetime import datetime, timedelta, timezone
import json, os, time, sys, threading, pathlib

# --- Configurazione Variabili d'Ambiente ---
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
KAFKA_CA = os.getenv("KAFKA_CA", "/etc/ssl/certs/kafka/ca.crt")
TOPIC = os.getenv("KAFKA_TOPIC", "student-events")
MONGO_URI = os.environ["MONGO_URI"]

# Configurazione del Batching (Bulk Processing)
BUFFER_SIZE = int(os.getenv("CONSUMER_BUFFER_SIZE", "20"))   # flush ogni N eventi
FLUSH_TIMEOUT = int(os.getenv("CONSUMER_FLUSH_TIMEOUT", "5"))  # o flush ogni X secondi

# File di Heartbeat per la Liveness Probe di Kubernetes
HEARTBEAT_FILE = "/tmp/heartbeat"

# --- Connessione Mongo (Retry Loop) ---
def get_mongo_collection():
    retries = 0
    while True:
        try:
            print(f"[CONSUMER] Tentativo connessione MongoDB... ({retries+1})", flush=True)
            client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            client.admin.command('ping')
            print("[CONSUMER] Connesso a MongoDB!", flush=True)
            return client.student_events.events
        except (ServerSelectionTimeoutError, Exception) as e:
            retries += 1
            print(f"[CONSUMER] MongoDB non pronto ({e}). Attendo 5 secondi...", flush=True)
            time.sleep(5)

# --- Connessione Kafka (Retry Loop) ---
def get_kafka_consumer():
    retries = 0
    while True:
        try:
            print(f"[CONSUMER] Tentativo connessione Kafka... ({retries+1})", flush=True)
            consumer = KafkaConsumer(
                TOPIC,
                bootstrap_servers=KAFKA_BOOTSTRAP,
                security_protocol="SASL_SSL" if SASL_USERNAME else "PLAINTEXT",
                sasl_mechanism="SCRAM-SHA-512" if SASL_USERNAME else None,
                sasl_plain_username=SASL_USERNAME if SASL_USERNAME else None,
                sasl_plain_password=SASL_PASSWORD if SASL_USERNAME else None,
                ssl_cafile=KAFKA_CA if SASL_USERNAME else None,
                auto_offset_reset='earliest',
                group_id='db-consumer-group',
                enable_auto_commit=False,  # relativo a at-least-once
                value_deserializer=lambda v: json.loads(v.decode('utf-8'))
            )
            print("[CONSUMER] Consumer Kafka pronto e sottoscritto!", flush=True)
            return consumer
        except NoBrokersAvailable:
            retries += 1
            print(f"[CONSUMER] Kafka non pronto. Attendo 5 secondi...", flush=True)
            time.sleep(5)
        except Exception as e:
            print(f"[CONSUMER] Errore Kafka generico: {e}. Riprovo tra 5s...", flush=True)
            time.sleep(5)

# --- Setup ---
collection = get_mongo_collection()
consumer = get_kafka_consumer()

# heartbeat loop (per liveness probe)
def heartbeat_loop():
    p = pathlib.Path(HEARTBEAT_FILE)
    while True:
        try:
            # scrivo semplicemente qualcosa per aggiornare il mtime
            p.write_bytes(b"ok")
        except Exception as e:
            print(f"[HEARTBEAT] write failed: {e}", flush=True)
        time.sleep(5)

hb_thread = threading.Thread(target=heartbeat_loop, daemon=True)
hb_thread.start()

# Buffer locale per il Bulk Insert
buffer = []
last_flush = datetime.utcnow()

def log_event(event):
    t = event.get("type")
    u = event.get("user_id")
    eid = event.get("event_id")
    print(f"[PROCESSED] event_id={eid} Type: {t}, User: {u}", flush=True)

def flush_buffer_and_commit():
    """
    Tenta il bulk insert e, se avvenuto con successo, esegue commit.
    """
    global buffer, last_flush
    if not buffer:
        return True

    # Prepara i documenti
    docs = []
    for ev in buffer:
        eid = ev.get("event_id") or ev.get("eventId") or None
        if not eid:
            eid = str(datetime.utcnow().timestamp()) + "-" + (ev.get("user_id","unknown"))
            ev["event_id"] = eid
        # Aggiungi ingest timestamp (utile per metriche)
        ev["_ingest_ts"] = datetime.utcnow().replace(tzinfo=timezone.utc)
        doc = {"_id": eid, **ev}
        docs.append(doc)

    try:
        # Scrittura su DB
        collection.insert_many(docs, ordered=False)

        print(f"[FLUSH] Inseriti {len(docs)} documenti su MongoDB:", flush=True)
        for d in docs:
            print(f"   -> Processed: {d.get('user_id')}", flush=True)

        buffer = []
        last_flush = datetime.utcnow()
        consumer.commit()
        return True
    
    except errors.BulkWriteError as bwe:
        # Gestione Idempotenza: Se ci sono duplicati, Mongo lancia questo errore.
        # i nuovi sono scritti, i doppi scartati
        panic = False
        for err in bwe.details.get('writeErrors', []):
            if err.get('code') != 11000:
                print(f"[FATAL] Errore scrittura non gestito: {err}", flush=True)
                panic = True
        
        if panic:
            print("[CONSUMER] Abort commit a causa di errori gravi DB.", flush=True)
            return False 

        print(f"[WARN] Duplicati gestiti: {bwe.details.get('nInserted')} inseriti", flush=True)
        for d in docs: print(f"   -> Processed (o duplicato): {d.get('user_id')}", flush=True)
        
        buffer = []
        last_flush = datetime.utcnow()
        consumer.commit()
        return True
        
    except Exception as e:
        print(f"[ERROR] Flush fallito: {e}", flush=True)
        return False

# --- Ciclo Principale di Consumo ---
consumer.subscribe([TOPIC])

print(f"[CONSUMER] Iniziato loop di consumo (Polling)...", flush=True)

try:
    while True:
        # 1. POLL: Legge messaggi per 1 secondo (1000ms)
        msg_pack = consumer.poll(timeout_ms=1000)

        for topic_partition, messages in msg_pack.items():
            for message in messages:
                buffer.append(message.value)

        # 2. CONTROLLI DI FLUSH (Eseguiti anche se non arrivano messaggi)
        now = datetime.utcnow()

        buffer_full = len(buffer) >= BUFFER_SIZE
        timeout_reached = (now - last_flush).total_seconds() >= FLUSH_TIMEOUT

        if buffer and (buffer_full or timeout_reached):
            ok = flush_buffer_and_commit()
            if not ok:
                print("[CONSUMER] Flush fallito, riprovo al prossimo giro...", flush=True)
                time.sleep(5)

except KeyboardInterrupt:
    print("Shutdown by user", flush=True)
    # Tenta un ultimo flush prima di chiudere
    flush_buffer_and_commit()
    sys.exit(0)
except Exception as e:
    print(f"[FATAL] {e}", flush=True)
    sys.exit(1)

==================== FINE FILE: src/consumer/consumer.py ====================

==================== FILE: src/consumer/requirements.txt ====================
kafka-python
pymongo
==================== FINE FILE: src/consumer/requirements.txt ====================

==================== FILE: src/consumer/consumer_V1.py ====================
# consumer.py (bulk + idempotency + flush timeout + heartbeat + _ingest_ts)
from kafka import KafkaConsumer
from kafka.errors import NoBrokersAvailable
from pymongo import MongoClient, errors
from pymongo.errors import ServerSelectionTimeoutError
from datetime import datetime, timedelta, timezone
import json, os, time, sys, threading, pathlib

# --- Configurazione Variabili d'Ambiente ---
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
KAFKA_CA = os.getenv("KAFKA_CA", "/etc/ssl/certs/kafka/ca.crt")
TOPIC = os.getenv("KAFKA_TOPIC", "student-events")
MONGO_URI = os.environ["MONGO_URI"]

# Configurazione del Batching (Bulk Processing)
BUFFER_SIZE = int(os.getenv("CONSUMER_BUFFER_SIZE", "20"))   # flush ogni N eventi
FLUSH_TIMEOUT = int(os.getenv("CONSUMER_FLUSH_TIMEOUT", "5"))  # o flush ogni X secondi

# File di Heartbeat per la Liveness Probe di Kubernetes
HEARTBEAT_FILE = "/tmp/heartbeat"

# --- Connessione Mongo (Retry Loop) ---
def get_mongo_collection():
    retries = 0
    while True:
        try:
            print(f"[CONSUMER] Tentativo connessione MongoDB... ({retries+1})", flush=True)
            client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            client.admin.command('ping')
            print("[CONSUMER] Connesso a MongoDB!", flush=True)
            return client.student_events.events
        except (ServerSelectionTimeoutError, Exception) as e:
            retries += 1
            print(f"[CONSUMER] MongoDB non pronto ({e}). Attendo 5 secondi...", flush=True)
            time.sleep(5)

# --- Connessione Kafka (Retry Loop) ---
def get_kafka_consumer():
    retries = 0
    while True:
        try:
            print(f"[CONSUMER] Tentativo connessione Kafka... ({retries+1})", flush=True)
            consumer = KafkaConsumer(
                TOPIC,
                bootstrap_servers=KAFKA_BOOTSTRAP,
                security_protocol="SASL_SSL" if SASL_USERNAME else "PLAINTEXT",
                sasl_mechanism="SCRAM-SHA-512" if SASL_USERNAME else None,
                sasl_plain_username=SASL_USERNAME if SASL_USERNAME else None,
                sasl_plain_password=SASL_PASSWORD if SASL_USERNAME else None,
                ssl_cafile=KAFKA_CA if SASL_USERNAME else None,
                auto_offset_reset='earliest',
                group_id='db-consumer-group',
                enable_auto_commit=False,  # relativo a at-least-once
                value_deserializer=lambda v: json.loads(v.decode('utf-8'))
            )
            print("[CONSUMER] Consumer Kafka pronto e sottoscritto!", flush=True)
            return consumer
        except NoBrokersAvailable:
            retries += 1
            print(f"[CONSUMER] Kafka non pronto. Attendo 5 secondi...", flush=True)
            time.sleep(5)
        except Exception as e:
            print(f"[CONSUMER] Errore Kafka generico: {e}. Riprovo tra 5s...", flush=True)
            time.sleep(5)

# --- Setup ---
collection = get_mongo_collection()
consumer = get_kafka_consumer()

# heartbeat loop (per liveness probe)
def heartbeat_loop():
    p = pathlib.Path(HEARTBEAT_FILE)
    while True:
        try:
            # scrivo semplicemente qualcosa per aggiornare il mtime
            p.write_bytes(b"ok")
        except Exception as e:
            print(f"[HEARTBEAT] write failed: {e}", flush=True)
        time.sleep(5)

hb_thread = threading.Thread(target=heartbeat_loop, daemon=True)
hb_thread.start()

# Buffer locale per il Bulk Insert
buffer = []
last_flush = datetime.utcnow()

def log_event(event):
    t = event.get("type")
    u = event.get("user_id")
    eid = event.get("event_id")
    print(f"[PROCESSED] event_id={eid} Type: {t}, User: {u}", flush=True)

def flush_buffer_and_commit():
    """
    Tenta il bulk insert e, se avvenuto con successo, esegue commit.
    """
    global buffer, last_flush
    if not buffer:
        return True

    # Prepara i documenti
    docs = []
    for ev in buffer:
        eid = ev.get("event_id") or ev.get("eventId") or None
        if not eid:
            eid = str(datetime.utcnow().timestamp()) + "-" + (ev.get("user_id","unknown"))
            ev["event_id"] = eid
        # Aggiungi ingest timestamp (utile per metriche)
        ev["_ingest_ts"] = datetime.utcnow().replace(tzinfo=timezone.utc)
        doc = {"_id": eid, **ev}
        docs.append(doc)

    try:
        # Scrittura su DB
        collection.insert_many(docs, ordered=False)

        print(f"[FLUSH] Inseriti {len(docs)} documenti su MongoDB:", flush=True)
        for d in docs:
            print(f"   -> Processed: {d.get('user_id')}", flush=True)

        buffer = []
        last_flush = datetime.utcnow()
        consumer.commit()
        return True

    except errors.BulkWriteError as bwe:
        # Gestione Idempotenza: Se ci sono duplicati, Mongo lancia questo errore.
        # i nuovi sono scritti, i doppi scartati
        inserted = bwe.details.get('nInserted') if bwe.details else None
        print(f"[WARN] BulkWriteError (possibili duplicati): inserted={inserted}", flush=True)
        for d in docs:
            print(f"   -> Processed (o duplicato): {d.get('user_id')}", flush=True)

        buffer = []
        last_flush = datetime.utcnow()
        consumer.commit()
        return True

    except Exception as e:
        print(f"[ERROR] Errore insert_many: {e}", flush=True)
        return False

# --- Ciclo Principale di Consumo ---
consumer.subscribe([TOPIC])

print(f"[CONSUMER] Iniziato loop di consumo (Polling)...", flush=True)

try:
    while True:
        # 1. POLL: Legge messaggi per 1 secondo (1000ms)
        msg_pack = consumer.poll(timeout_ms=1000)

        for topic_partition, messages in msg_pack.items():
            for message in messages:
                buffer.append(message.value)

        # 2. CONTROLLI DI FLUSH (Eseguiti anche se non arrivano messaggi)
        now = datetime.utcnow()

        buffer_full = len(buffer) >= BUFFER_SIZE
        timeout_reached = (now - last_flush).total_seconds() >= FLUSH_TIMEOUT

        if buffer and (buffer_full or timeout_reached):
            ok = flush_buffer_and_commit()
            if not ok:
                print("[CONSUMER] Flush fallito, riprovo al prossimo giro...", flush=True)
                time.sleep(5)

except KeyboardInterrupt:
    print("Shutdown by user", flush=True)
    # Tenta un ultimo flush prima di chiudere
    flush_buffer_and_commit()
    sys.exit(0)
except Exception as e:
    print(f"[FATAL] {e}", flush=True)
    sys.exit(1)

==================== FINE FILE: src/consumer/consumer_V1.py ====================

==================== FILE: k8s/00-infrastructure/kafka-cluster.yaml ====================
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
# Node pool dedicato ai controller: questi nodi partecipano al quorum
# del controller (KRaft) e gestiscono il metadata log del cluster.
metadata:
  name: controller
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  # Numero di repliche dei controller. Deve essere scelto per garantire
  # tolleranza ai guasti del quorum di controllo.
  replicas: 2
  roles:
    - controller
  storage:
    # JBOD permette più volumi logici per nodo; qui è usato un singolo volume
    type: jbod
    volumes:
      - id: 0
        # Persistent Claim: mantiene i dati oltre il ciclo di vita del pod
        type: persistent-claim
        size: 5Gi
        # Indica che questo volume conterrà i metadata condivisi KRaft
        kraftMetadata: shared
        # Impedisce la cancellazione automatica del PVC alla rimozione del nodo
        deleteClaim: false
---
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
# Node pool per i broker: questi nodi memorizzano i log dei topic e forniscono
# gli endpoint ai client (producer/consumer).
metadata:
  name: broker
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  # Numero di broker che saranno creati per questa pool
  replicas: 2
  roles:
    - broker
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 5Gi
        # Nota: lo storage dei broker è destinato ai log dei topic (dati).
        # Non è abilitato kraftMetadata per i broker perché i metadata
        # sono gestiti dai controller. Se si volesse condividere metadata
        # sullo stesso volume, abilitare kraftMetadata: shared.
        # kraftMetadata: shared
        deleteClaim: false
---
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  # Definizione principale del cluster Kafka gestito da Strimzi
  name: uni-it-cluster
  namespace: kafka
spec:
  kafka:
    # Versione del broker Kafka
    version: 4.1.0
    # Versione del metadata (KRaft) utilizzata internamente
    metadataVersion: 4.1-IV1
    listeners:
      - name: plain
        # Listener interno non cifrato per comunicazioni interne
        port: 9092
        type: internal
        tls: false
      - name: tls
        # Listener interno cifrato TLS per connessioni sicure
        port: 9093
        type: internal
        tls: true
        authentication:
          # Autenticazione SASL: SCRAM-SHA-512
          type: scram-sha-512
    config:
      # Configurazioni di replica e durabilità. I valori qui impostati
      # devono essere coerenti con il numero di repliche e le esigenze di HA.
      # Fattore di replicazione per i topic di offset e transazioni
      offsets.topic.replication.factor: 2
      transaction.state.log.replication.factor: 2
      default.replication.factor: 2
      # Minimo di In-Sync Replicas: valori più alti aumentano durabilità
      transaction.state.log.min.isr: 1
      min.insync.replicas: 1
  entityOperator:
    # TopicOperator e UserOperator automatizzano la gestione di topic e utenti
    topicOperator: {}
    userOperator: {}
---
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: uni-it-cluster
  namespace: kafka
spec:
  kafka:
    version: 4.1.0
    metadataVersion: 4.1-IV1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
        authentication:
          type: scram-sha-512
    config:
      #Aumenta il fattore di replicazione a 3 per una maggiore tolleranza ai guasti
      offsets.topic.replication.factor: 2
      transaction.state.log.replication.factor: 2
      default.replication.factor: 2
      transaction.state.log.min.isr: 1 
      min.insync.replicas: 1
  entityOperator:
    topicOperator: {}
    userOperator: {}

==================== FINE FILE: k8s/00-infrastructure/kafka-cluster.yaml ====================

==================== FILE: k8s/00-infrastructure/kafka-topic.yaml ====================
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: student-events
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  partitions: 3
  replicas: 2  # 2 --> se un broker muore, l'altro ha i dati
  config:
    retention.ms: 604800000
    segment.bytes: 1073741824
    min.insync.replicas: 1 
==================== FINE FILE: k8s/00-infrastructure/kafka-topic.yaml ====================

==================== FILE: k8s/00-infrastructure/users.yaml ====================
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: producer-user
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  authentication:
    type: scram-sha-512
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: consumer-user
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  authentication:
    type: scram-sha-512
==================== FINE FILE: k8s/00-infrastructure/users.yaml ====================

==================== FILE: k8s/03-gateway/jwt-plugin-metrics.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
  namespace: metrics
plugin: jwt
config:
  claims_to_verify:
    - exp
==================== FINE FILE: k8s/03-gateway/jwt-plugin-metrics.yaml ====================

==================== FILE: k8s/03-gateway/jwt-consumer.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongConsumer
metadata:
  name: exam-client
  namespace: kafka
  annotations:
    kubernetes.io/ingress.class: kong
username: exam-client
credentials:
  - exam-client-jwt
==================== FINE FILE: k8s/03-gateway/jwt-consumer.yaml ====================

==================== FILE: k8s/03-gateway/rate-limit-plugin.yaml ====================
# Policy per il namespace KAFKA (Producer)
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limit
  namespace: kafka
plugin: rate-limiting
config:
  second: 3        # 3 richieste al secondo
  policy: local
  hide_client_headers: false # true per vedere gli header di debug
---
# Policy per il namespace METRICS (Metrics Service)
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limit
  namespace: metrics
plugin: rate-limiting
config:
  second: 20       # 20 richieste al secondo
  policy: local
  hide_client_headers: false
==================== FINE FILE: k8s/03-gateway/rate-limit-plugin.yaml ====================

==================== FILE: k8s/03-gateway/jwt-plugin-kafka.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
  namespace: kafka
plugin: jwt
config:
  claims_to_verify:
    - exp
==================== FINE FILE: k8s/03-gateway/jwt-plugin-kafka.yaml ====================

==================== FILE: k8s/03-gateway/kong-cors-plugin.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongClusterPlugin
metadata:
  name: cors-plugin
  annotations:
    kubernetes.io/ingress.class: kong
  labels:
    global: "true"
plugin: cors
config:
  origins:
    - "*"
  methods:
    - GET
    - POST
    - OPTIONS
  headers:
    - Authorization
    - Content-Type
  max_age: 3600
==================== FINE FILE: k8s/03-gateway/kong-cors-plugin.yaml ====================

==================== FILE: k8s/03-gateway/metrics-ingress.yaml ====================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: metrics-ingress
  namespace: metrics
  annotations:
    konghq.com/plugins: jwt-auth, rate-limit, cors-plugin
    konghq.com/strip-path: "false"
    kubernetes.io/ingress.class: kong
spec:
  ingressClassName: kong
  rules:
    - host: metrics.192.168.49.2.nip.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: metrics-service
                port:
                  number: 5001
==================== FINE FILE: k8s/03-gateway/metrics-ingress.yaml ====================

==================== FILE: k8s/03-gateway/producer-ingress.yaml ====================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: producer-ingress
  namespace: kafka
  annotations:
    konghq.com/plugins: jwt-auth, rate-limit, cors-plugin
    kubernetes.io/ingress.class: kong
spec:
  ingressClassName: kong
  rules:
  - host: producer.192.168.49.2.nip.io
    http:
      paths:
        - path: /event
          pathType: Prefix
          backend:
            service:
              name: producer-service
              port:
                number: 5000
==================== FINE FILE: k8s/03-gateway/producer-ingress.yaml ====================

==================== FILE: k8s/01-security/mongo-network-policy.yaml ====================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-mongo-access
  namespace: kafka
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: mongo-mongodb # Label standard del chart Bitnami
  policyTypes:
  - Ingress
  ingress:
  # Regola 1: Permette accesso dal Consumer (stesso namespace 'kafka')
  - from:
    - podSelector:
        matchLabels:
          app: consumer
    ports:
    - protocol: TCP
      port: 27017

  # Regola 2: Permette accesso dal Metrics Service (namespace 'metrics')
  - from:
    - namespaceSelector:
        matchLabels:
          name: metrics   # Cerca il namespace con label name=metrics
      podSelector:
        matchLabels:
          app: metrics-service
    ports:
    - protocol: TCP
      port: 27017
==================== FINE FILE: k8s/01-security/mongo-network-policy.yaml ====================

==================== FILE: k8s/01-security/jwt-credential.yaml ====================
apiVersion: v1
kind: Secret
metadata:
  name: exam-client-jwt
  namespace: kafka
  labels:
    konghq.com/credential: jwt
type: Opaque
stringData:
  key: "exam-client-key"
  algorithm: "HS256"
  secret: "supersecret"
==================== FINE FILE: k8s/01-security/jwt-credential.yaml ====================

==================== FILE: k8s/02-apps/consumer-deployment.yaml ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: consumer
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: consumer
  template:
    metadata:
      labels:
        app: consumer
    spec:
      containers:
      - name: consumer
        image: consumer:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: KAFKA_BOOTSTRAP
          value: "uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093"
        - name: SASL_USERNAME
          value: "consumer-user"   
        - name: SASL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: consumer-user
              key: password         
        - name: KAFKA_CA
          value: "/etc/ssl/certs/kafka/ca.crt"
        - name: MONGO_URI
          valueFrom:
            secretKeyRef:
              name: mongo-creds
              key: MONGO_URI
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - test -f /tmp/heartbeat && [ $(($(date +%s) - $(date -r /tmp/heartbeat +%s))) -lt 30 ]
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        volumeMounts:
        - name: kafka-ca
          mountPath: /etc/ssl/certs/kafka
          readOnly: true
      volumes:
      - name: kafka-ca
        secret:
          secretName: kafka-ca-cert

==================== FINE FILE: k8s/02-apps/consumer-deployment.yaml ====================

==================== FILE: k8s/02-apps/producer-deployment.yaml ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: producer
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: producer
  template:
    metadata:
      labels:
        app: producer
    spec:
      containers:
      - name: producer
        image: producer:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: KAFKA_BOOTSTRAP
          value: "uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093"
        - name: SASL_USERNAME
          value: "producer-user"  
        - name: SASL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: producer-user
              key: password         
        - name: KAFKA_CA
          value: "/etc/ssl/certs/kafka/ca.crt"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
        ports:
        - containerPort: 5000
        #probes
        readinessProbe:
          httpGet:
            path: /healthz
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /healthz
            port: 5000
          initialDelaySeconds: 15
          periodSeconds: 10
          failureThreshold: 6   # Sii paziente prima di ucciderlo (potrebbe essere solo lento)
        volumeMounts:
        - name: kafka-ca
          mountPath: /etc/ssl/certs/kafka
          readOnly: true
      volumes:
      - name: kafka-ca
        secret:
          secretName: kafka-ca-cert
---
apiVersion: v1
kind: Service
metadata:
  name: producer-service
  namespace: kafka
spec:
  selector:
    app: producer
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
  type: ClusterIP

==================== FINE FILE: k8s/02-apps/producer-deployment.yaml ====================

==================== FILE: k8s/02-apps/hpa.yaml ====================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: producer-hpa
  namespace: kafka
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: producer
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50   # scala sopra il 50% di CPU media
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: consumer-hpa
  namespace: kafka
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: consumer
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-service-hpa
  namespace: metrics
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics-service
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

==================== FINE FILE: k8s/02-apps/hpa.yaml ====================

==================== FILE: k8s/02-apps/metrics-deployment.yaml ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-service
  namespace: metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics-service
  template:
    metadata:
      labels:
        app: metrics-service
    spec:
      containers:
      - name: metrics-service
        image: metrics-service:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: MONGO_URI
          valueFrom:
            secretKeyRef:
              name: mongo-creds
              key: MONGO_URI
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        ports:
        - containerPort: 5001
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
        #probes
        readinessProbe:
          httpGet:
            path: /healthz
            port: 5001
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /healthz
            port: 5001
          initialDelaySeconds: 15
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-service
  namespace: metrics
spec:
  selector:
    app: metrics-service
  ports:
    - protocol: TCP
      port: 5001
      targetPort: 5001
  type: ClusterIP

==================== FINE FILE: k8s/02-apps/metrics-deployment.yaml ====================

==================== FILE: demoV3/02_resilience.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "FASE 2: RESILIENZA & FAULT TOLERANCE"

# Funzione helper per contare documenti
get_mongo_count() {
    PASS=$(kubectl get secret -n kafka mongo-mongodb -o jsonpath="{.data.mongodb-root-password}" | base64 -d)
    kubectl exec -n kafka deployment/mongo-mongodb -c mongodb -- mongosh -u root -p $PASS --authenticationDatabase admin --eval 'db.getSiblingDB("student_events").events.countDocuments()' --quiet 2>/dev/null | tr -d '\r\n '
}

INIT_COUNT=$(get_mongo_count)
echo -e "Documenti iniziali nel DB: ${YELLOW}$INIT_COUNT${NC}"

if [ "$INIT_COUNT" != "0" ]; then
    error "Database non vuoto. Riprovare."
    exit 1
fi

# --- SCENARIO 1: DURABILITÀ POST-INGESTIONE ---
step "Scenario 1: Durabilità Post-Ingestione (Consumer Down)"
log "Dimostro che i dati sopravvivono al downtime del Consumer..."

log "1. Spegnimento Consumer..."
run kubectl scale deploy/consumer -n kafka --replicas=0
kubectl wait --for=delete pod -l app=consumer -n kafka --timeout=60s >/dev/null 2>&1
echo -e "${RED}Consumer OFFLINE${NC}"

log "2. Invio 5 eventi (vanno in Kafka backlog)..."
for i in {1..5}; do
    HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$BASE_URL/event/login" \
        -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
        -d "{\"user_id\":\"offline-msg-$i\"}")
    echo -e "  Evento $i → ${GREEN}HTTP $HTTP_CODE${NC}"
done

log "3. Verifica DB (deve essere ancora vuoto)..."
MID_COUNT=$(get_mongo_count)
echo -e "Documenti nel DB: ${YELLOW}$MID_COUNT${NC} (atteso: 0)"

log "4. Ripristino Consumer..."
run kubectl scale deploy/consumer -n kafka --replicas=1
echo -e "${YELLOW}Attendo avvio e processamento backlog (15s)...${NC}"
sleep 15

log "5. Verifica finale DB..."
FINAL_COUNT=$(get_mongo_count)
echo -e "Documenti attesi: ${CYAN}5${NC}"
echo -e "Documenti trovati: ${YELLOW}$FINAL_COUNT${NC}"

if [ "$FINAL_COUNT" == "5" ]; then
    echo -e "\n${GREEN}✓✓✓ SUCCESSO: Tutti i dati recuperati!${NC}"
    echo -e "${GREEN}Dimostrato: Durabilità Post-Ingestione${NC}"
else
    echo -e "\n${RED}✗ ERRORE: Trovati solo $FINAL_COUNT/5 documenti${NC}"
    echo -e "${YELLOW}Suggerimento: kubectl logs -n kafka -l app=consumer --tail=50${NC}"
fi

pause

# --- SCENARIO 2: HA PRODUCER ---
step "Scenario 2: High Availability Producer (Zero Downtime)"
log "Dimostro che il sistema rimane disponibile anche durante pod failures..."

# Scala a 2 repliche
log "1. Scaling Producer a 2 repliche..."
kubectl scale deploy/producer -n kafka --replicas=2 >/dev/null
kubectl rollout status deploy/producer -n kafka --timeout=60s >/dev/null
echo -e "${GREEN}✓ 2 repliche attive${NC}"

# Identifica pod da killare
POD_NAME=$(kubectl get pods -n kafka -l app=producer -o jsonpath="{.items[0].metadata.name}")
log "2. Target: $POD_NAME"

# Kill + richiesta simultanea
log "3. Kill pod DURANTE invio richiesta..."
kubectl delete pod $POD_NAME -n kafka --grace-period=0 --force >/dev/null 2>&1 &
sleep 0.3

HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$BASE_URL/event/login" \
    -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
    -d '{"user_id": "ha-check"}')

if [[ "$HTTP_CODE" == "200" || "$HTTP_CODE" == "202" ]]; then
    echo -e "\n${GREEN}✓✓✓ SUCCESSO: HTTP $HTTP_CODE ricevuto${NC}"
    echo -e "${GREEN}Kong ha reindirizzato traffico alla replica sana${NC}"
else
    echo -e "\n${RED}✗ FALLITO: HTTP $HTTP_CODE${NC}"
fi

# Cleanup
kubectl scale deploy/producer -n kafka --replicas=1 >/dev/null

pause

# --- SCENARIO 3: HA KAFKA BROKER ---
step "Scenario 3: Kafka Broker Failover (Infrastructure Resilience)"
log "Dimostro che Kafka tollera la perdita di 1 broker su 2..."

BROKER_POD="uni-it-cluster-broker-0"
log "1. Target Broker: $BROKER_POD"

# Conta messaggi prima del test
BEFORE_COUNT=$(get_mongo_count)
log "2. Documenti nel DB PRIMA del test: $BEFORE_COUNT"

# Avvia invio continuo in background
echo -e "${YELLOW}3. Avvio invio 30 messaggi in background...${NC}"
(
  for i in {1..30}; do
     curl -s -o /dev/null -X POST "$BASE_URL/event/login" \
     -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
     -d "{\"user_id\":\"ha-kafka-$i\"}"
     sleep 0.5
  done
) &
BG_PID=$!

sleep 3

# KILL BROKER nel mezzo
log "4. >>> KILLING KAFKA BROKER: $BROKER_POD <<<"
kubectl delete pod $BROKER_POD -n kafka --grace-period=0 --force >/dev/null 2>&1

echo -e "${RED}Broker terminato forzatamente!${NC}"
echo -e "${YELLOW}Kafka sta eseguendo failover automatico...${NC}"
echo -e "${YELLOW}Producer utilizza retry + replica follower...${NC}"

# Attendi fine invio
wait $BG_PID
echo -e "${GREEN}Invio completato (30 messaggi).${NC}"

# Attendi processamento Consumer
log "5. Attendo processamento Consumer (10s)..."
sleep 10

# Verifica finale
AFTER_COUNT=$(get_mongo_count)
DELTA=$((AFTER_COUNT - BEFORE_COUNT))

echo -e "\n${CYAN}=== RISULTATI ===${NC}"
echo -e "Messaggi nel DB PRIMA: ${YELLOW}$BEFORE_COUNT${NC}"
echo -e "Messaggi nel DB DOPO:  ${YELLOW}$AFTER_COUNT${NC}"
echo -e "Messaggi aggiunti:     ${CYAN}$DELTA${NC} (attesi: 30)"

if [ $DELTA -eq 30 ]; then
    echo -e "\n${GREEN}✓✓✓ PERFETTO: Tutti i 30 messaggi persistiti!${NC}"
    echo -e "${GREEN}Dimostrato: Kafka HA con Replication Factor 2${NC}"
elif [ $DELTA -ge 25 ]; then
    echo -e "\n${YELLOW}⚠ PARZIALE: $DELTA/30 messaggi (accettabile durante failover)${NC}"
else
    echo -e "\n${RED}✗ FALLITO: Solo $DELTA/30 messaggi${NC}"
fi

# Verifica stato broker
log "6. Stato cluster Kafka:"
kubectl get pods -n kafka | grep broker
==================== FINE FILE: demoV3/02_resilience.sh ====================

==================== FILE: demoV3/05_governance.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "FASE 5: GOVERNANCE (RATE LIMITING)"

# --- PARTE 1 ---
step "SCENARIO A: Test CON Rate Limiting (3 req/sec)"

log "1. Applicazione Policy..."
cat <<EOF | kubectl apply -f - >/dev/null
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limit
  namespace: kafka
plugin: rate-limiting
config:
  second: 3
  policy: local
EOF

log "2. Attivazione su Ingress..."
kubectl annotate ingress producer-ingress -n kafka --overwrite \
  konghq.com/plugins="cors-plugin,jwt-auth,rate-limit" >/dev/null

log "Attendo propagazione su Kong (10s)..."
sleep 10

log "3. Esecuzione Flood Test (20 richieste)..."
echo -e "${YELLOW}[Flood in corso...]${NC}"

results=()
for i in {1..20}; do
  CODE=$(curl -s -o /dev/null -w "%{http_code}" \
  -X POST "$BASE_URL/event/login" \
  -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
  -d "{\"user_id\":\"flood-$i\"}")
  results+=($CODE)
done

echo ""
count_blocked=0
for i in "${!results[@]}"; do
  req_num=$((i+1))
  code="${results[$i]}"
  if [ "$code" == "429" ]; then
    echo -e "Req $req_num: ${RED}BLOCKED (429)${NC}"
    ((count_blocked++))
  else
    echo -e "Req $req_num: ${GREEN}OK ($code)${NC}"
  fi
done

if [ $count_blocked -gt 0 ]; then
    echo -e "\n${GREEN}✅ SUCCESSO: $count_blocked richieste bloccate.${NC}"
else
    echo -e "\n${RED}❌ FALLITO: Nessun blocco rilevato.${NC}"
fi

pause

# --- PARTE 2 ---
step "SCENARIO B: Test SENZA Rate Limiting (Verifica)"

log "1. Rimozione Policy..."
kubectl annotate ingress producer-ingress -n kafka --overwrite konghq.com/plugins="cors-plugin,jwt-auth" >/dev/null

log "Attendo aggiornamento Kong (10s)..."
sleep 10

log "2. Esecuzione Flood Test (20 richieste)..."
echo -e "${YELLOW}[Flood in corso...]${NC}"

results_clean=()
for i in {1..20}; do
  CODE=$(curl -s -o /dev/null -w "%{http_code}" \
  -X POST "$BASE_URL/event/login" \
  -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
  -d "{\"user_id\":\"flood-clean-$i\"}")
  results_clean+=($CODE)
done

blocked_clean=0
for code in "${results_clean[@]}"; do
  if [ "$code" == "429" ]; then ((blocked_clean++)); fi
done

if [ $blocked_clean -eq 0 ]; then
    echo -e "${GREEN}✅ VERIFICA OK: Tutte le richieste passate (200 OK).${NC}"
else
    echo -e "${RED}❌ ERRORE: Ancora $blocked_clean blocchi!${NC}"
fi

# Cleanup solo del plugin (NON dell'HPA)
kubectl delete kongplugin rate-limit -n kafka >/dev/null 2>&1
==================== FINE FILE: demoV3/05_governance.sh ====================

==================== FILE: demoV3/03_scalability.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "FASE 3: SCALABILITÀ & LOAD BALANCING"

# --- FIX: DISATTIVA HPA ---
step "0. Preparazione Ambiente"
log "Disattivo HPA per test manuale..."
kubectl delete hpa --all -n kafka >/dev/null 2>&1
kubectl delete hpa --all -n metrics >/dev/null 2>&1
echo -e "${GREEN}✓ HPA disattivato${NC}"

# --- TEST 1: LOAD BALANCING ---
step "1. Test Load Balancing (Round Robin)"
log "Dimostro che Kong distribuisce il carico su N repliche..."

log "1. Scaling a 3 repliche..."
run kubectl scale deploy/producer -n kafka --replicas=3
echo -e "${YELLOW}Attendo stabilizzazione (15s)...${NC}"
kubectl rollout status deploy/producer -n kafka --timeout=60s >/dev/null
sleep 5

log "2. Verifica endpoint Service..."
kubectl get endpoints producer-service -n kafka -o jsonpath='{.subsets[0].addresses[*].ip}' | tr ' ' '\n' | nl
echo ""

log "3. Invio 9 richieste consecutive (atteso: 3-3-3)..."
echo ""

declare -A pod_counter
for i in {1..9}; do
    RESPONSE=$(curl -s "$BASE_URL/event/login" \
        -H "Authorization: Bearer $TOKEN" \
        -H "Content-Type: application/json" \
        -d "{\"user_id\":\"lb-test-$i\"}")
    
    POD=$(echo $RESPONSE | grep -o '"processed_by":"[^"]*"' | cut -d'"' -f4)
    pod_counter[$POD]=$((${pod_counter[$POD]:-0} + 1))
    
    echo -e "Req #$i → ${CYAN}$POD${NC}"
done

echo -e "\n${CYAN}=== DISTRIBUZIONE ===${NC}"
for pod in "${!pod_counter[@]}"; do
    echo -e "  $pod: ${GREEN}${pod_counter[$pod]} richieste${NC}"
done

echo -e "\n${GREEN}✓ Load Balancing verificato${NC}"

pause

# --- TEST 2: THROUGHPUT COMPARISON ---
step "2. Throughput Comparison: 3 Repliche vs 1 Replica"
log "Misuro tempo per processare 500 richieste parallele..."

echo -e "\n${CYAN}TEST A: 3 Repliche (High Scale)${NC}"
echo -e "${YELLOW}Esecuzione in corso... (attendi ~20-30s)${NC}"

START_TIME=$(date +%s)
for i in {1..10}; do
  ( for j in {1..50}; do
      curl -s -o /dev/null -X POST "$BASE_URL/event/login" \
      -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
      -d '{"user_id":"perf-3rep"}'
    done ) &
done
wait
END_TIME=$(date +%s)
TIME_3REP=$((END_TIME - START_TIME))

echo -e "${GREEN}Completato in: ${TIME_3REP}s${NC}"

# Scaling down
log "Scaling down a 1 replica..."
run kubectl scale deploy/producer -n kafka --replicas=1
kubectl rollout status deploy/producer -n kafka --timeout=60s >/dev/null
sleep 5

echo -e "\n${CYAN}TEST B: 1 Replica (Low Scale)${NC}"
echo -e "${YELLOW}Esecuzione in corso... (attendi ~40-60s)${NC}"

START_TIME=$(date +%s)
for i in {1..10}; do
  ( for j in {1..50}; do
      curl -s -o /dev/null -X POST "$BASE_URL/event/login" \
      -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
      -d '{"user_id":"perf-1rep"}'
    done ) &
done
wait
END_TIME=$(date +%s)
TIME_1REP=$((END_TIME - START_TIME))

echo -e "${GREEN}Completato in: ${TIME_1REP}s${NC}"

# Confronto
echo -e "\n${CYAN}=== CONFRONTO THROUGHPUT ===${NC}"
echo -e "  3 Repliche: ${GREEN}${TIME_3REP}s${NC}"
echo -e "  1 Replica:  ${YELLOW}${TIME_1REP}s${NC}"

SPEEDUP=$(awk "BEGIN {printf \"%.2f\", $TIME_1REP / $TIME_3REP}")
echo -e "  Speedup:    ${CYAN}${SPEEDUP}x${NC}"

echo -e "\n${YELLOW}NOTA: Risultati dipendono da risorse Minikube disponibili${NC}"

if (( $(echo "$SPEEDUP > 1.5" | bc -l) )); then
    echo -e "${GREEN}✓ Scalabilità confermata (3 repliche ~${SPEEDUP}x più veloci)${NC}"
else
    echo -e "${YELLOW}⚠ Speedup limitato (possibile bottleneck Minikube o rete)${NC}"
fi

# --- TEST 2: THROUGHPUT ---
step "2. Throughput Test: 3 Repliche vs 1 Replica"
log "Lancio 500 richieste in parallelo per misurare il tempo di esecuzione."

step "TEST A: High Scale (3 Repliche)"
echo "Esecuzione in corso... (Attendere)"
# time misura quanto ci mette il blocco a finire
time (
  for i in {1..10}; do
    ( for j in {1..50}; do
        curl -s -o /dev/null -X POST "$BASE_URL/event/login" \
        -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
        -H "Connection: close" \
        -d "{\"user_id\":\"perf\"}"
      done ) & 
  done
  wait
)

step "Scaling Down a 1 Replica..."
run kubectl scale deploy/producer -n kafka --replicas=1
echo "Attendo shutdown dei pod in eccesso..."
kubectl rollout status deploy/producer -n kafka >/dev/null
sleep 5

step "TEST B: Low Scale (1 Replica)"
echo "Esecuzione in corso... (Attendere)"
time (
  for i in {1..10}; do
    ( for j in {1..50}; do
        curl -s -o /dev/null -X POST "$BASE_URL/event/login" \
        -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
        -H "Connection: close" \
        -d "{\"user_id\":\"perf\"}"
      done ) & 
  done
  wait
)

echo -e "\n${GREEN}Confronto completato: con 3 repliche il tempo 'real' dovrebbe essere inferiore.${NC}"
==================== FINE FILE: demoV3/03_scalability.sh ====================

==================== FILE: demoV3/06_functional_test.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "TEST FUNZIONALE: BUSINESS LOGIC (Happy Path)"

# Funzione helper per verificare i valori
assert_metric() {
    ENDPOINT=$1
    KEY=$2
    EXPECTED=$3
    DESC=$4

    # Esegue la chiamata e salva risposta e codice HTTP
    RESPONSE=$(curl -s -H "Authorization: Bearer $TOKEN" "$METRICS_URL$ENDPOINT")
    
    # Estrae il valore usando python (gestisce float e int)
    # Se la chiave è annidata o complessa, questo one-liner è flessibile
    ACTUAL=$(echo "$RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin).get('$KEY', 'null'))" 2>/dev/null)

    if [ "$ACTUAL" == "$EXPECTED" ] || [ "$ACTUAL" == "${EXPECTED}.0" ]; then
        echo -e "${GREEN}✅ $DESC: OK (Trovato: $ACTUAL)${NC}"
    else
        echo -e "${RED}❌ $DESC: FALLITO (Atteso: $EXPECTED, Trovato: $ACTUAL)${NC}"
        echo -e "   Risposta API: $RESPONSE"
    fi
}

# 1. Reset per avere dati puliti e controllabili
reset_db

step "1. Simulazione Attività Utenti"
log "Generazione traffico realistico (3 Utenti)..."

# --- UTENTE 1: MARIO (Studente Modello) ---
log "👤 Mario: Login, Download Slide, Quiz (30/30)."
run curl -s -o /dev/null -X POST "$BASE_URL/event/login" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"mario"}'
run curl -s -o /dev/null -X POST "$BASE_URL/event/download" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"mario", "materiale_id":"slide-k8s-pdf", "course_id":"cloud"}'
run curl -s -o /dev/null -X POST "$BASE_URL/event/quiz" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"mario", "quiz_id":"quiz-1", "course_id":"cloud", "score": 30}'

# --- UTENTE 2: LUIGI (Passa a pelo) ---
log "👤 Luigi: Login, Quiz (18/30)."
run curl -s -o /dev/null -X POST "$BASE_URL/event/login" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"luigi"}'
run curl -s -o /dev/null -X POST "$BASE_URL/event/quiz" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"luigi", "quiz_id":"quiz-1", "course_id":"cloud", "score": 18}'

# --- UTENTE 3: PEACH (Pianificatrice) ---
log "👤 Peach: Login, Prenotazione Esame."
run curl -s -o /dev/null -X POST "$BASE_URL/event/login" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"peach"}'
run curl -s -o /dev/null -X POST "$BASE_URL/event/exam" -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" -d '{"user_id":"peach", "esame_id":"appello-gennaio", "course_id":"cloud"}'

echo -e "${GREEN}>>> Eventi inviati. Attendo elaborazione asincrona (8s)...${NC}"
sleep 8

# --- VERIFICA METRICHE ---
step "2. Verifica Automatica Dashboard (Metrics Service)"
METRICS_URL="http://metrics.$IP.nip.io:$PORT/metrics"

# 1. Logins
# Atteso: 3 (Mario, Luigi, Peach)
assert_metric "/logins" "total_logins" "3" "Totale Login"

# 2. Quiz Success Rate
# Atteso: 100 (Mario 30 >= 18, Luigi 18 >= 18 -> Entrambi promossi)
assert_metric "/quiz/success-rate" "success_rate" "100" "Tasso Successo Quiz"

# 3. Average Score (Nuovo test!)
# Atteso: (30 + 18) / 2 = 24.
# Nota: La tua API ritorna una lista paginata [{"average_score": 24.0, ...}]
# Dobbiamo estrarre il primo elemento della lista 'data'
RESPONSE=$(curl -s -H "Authorization: Bearer $TOKEN" "$METRICS_URL/quiz/average-score")
AVG=$(echo "$RESPONSE" | python3 -c "import sys, json; data=json.load(sys.stdin).get('data', []); print(data[0]['average_score'] if data else '0')")

if [ "$AVG" == "24.0" ] || [ "$AVG" == "24" ]; then
    echo -e "${GREEN}✅ Media Voti: OK (Trovato: 24)${NC}"
else
    echo -e "${RED}❌ Media Voti: FALLITO (Atteso: 24, Trovato: $AVG)${NC}"
fi

echo -e "\n${GREEN}✅ Test Funzionale Completato.${NC}"
==================== FINE FILE: demoV3/06_functional_test.sh ====================

==================== FILE: demoV3/07_self_healing.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "FASE 7: SELF-HEALING & PROBES"

# --- TEST 1: READINESS (SAFE ROLLOUT) ---
step "Test 1: Producer Readiness (Protezione Deploy Bacato)"
log "Obiettivo: Verificare che un aggiornamento con configurazione errata venga bloccato."

# 1. Assicuriamoci di avere 1 replica stabile
kubectl scale deploy/producer -n kafka --replicas=1 >/dev/null
kubectl rollout status deploy/producer -n kafka --timeout=60s >/dev/null

log "1. Simulo deploy di una versione guasta (Kafka URL errato)..."
# Questo crea un NUOVO pod che fallirà la readiness probe
kubectl set env deployment/producer -n kafka KAFKA_BOOTSTRAP="fake-kafka:9092" >/dev/null

log "2. Attendo tentativo di rollout (15s)..."
sleep 15

log "3. Verifica Stato Pods:"
kubectl get pods -n kafka -l app=producer
echo ""

# Contiamo i pod NON pronti (Ready=0/1)
NOT_READY=$(kubectl get pods -n kafka -l app=producer -o jsonpath='{range .items[*]}{.status.containerStatuses[*].ready}{"\n"}{end}' | grep "false" | wc -l)

if [ "$NOT_READY" -ge 1 ]; then
    echo -e "${GREEN}✅ SUCCESSO: Il nuovo pod è stato marcato 'NotReady'. Traffico bloccato.${NC}"
else
    echo -e "${RED}❌ FALLITO: Tutti i pod sembrano pronti (la probe non ha funzionato).${NC}"
fi

# Ripristino
log "4. Rollback configurazione..."
kubectl rollout undo deployment/producer -n kafka >/dev/null
kubectl rollout status deploy/producer -n kafka --timeout=60s >/dev/null

pause

# --- TEST 2: LIVENESS (ZOMBIE KILLER) ---
step "Test 2: Consumer Liveness (Deadlock Detection)"
log "Obiettivo: Simulare un processo bloccato (Zombie) e verificare il riavvio."

CONSUMER_POD=$(kubectl get pod -n kafka -l app=consumer -o jsonpath='{.items[0].metadata.name}')
log "Target: $CONSUMER_POD"

log "1. Congelo il processo (SIGSTOP)..."
# Usiamo /bin/sh per lanciare kill
kubectl exec -n kafka $CONSUMER_POD -- /bin/sh -c "kill -STOP 1"

log "2. Attendo intervento Kubernetes (Max 100s)..."
log "(Logica: 30s validità + 3 check x 10s + tempi tecnici)"

# Loop di attesa attivo (Polling dello stato)
# Controlliamo ogni 5 secondi se il pod è cambiato o riavviato
RESTARTED=0
for i in {1..20}; do
    CURRENT_POD=$(kubectl get pod -n kafka -l app=consumer -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
    CURRENT_RESTARTS=$(kubectl get pod -n kafka -l app=consumer -o jsonpath='{.items[0].status.containerStatuses[0].restartCount}' 2>/dev/null)
    
    # Se il pod ha cambiato nome O i restart sono aumentati
    if [ "$CURRENT_POD" != "$CONSUMER_POD" ] || [ "$CURRENT_RESTARTS" -gt "0" ]; then
        RESTARTED=1
        echo -e "\n${GREEN}!!! Riavvio Rilevato !!!${NC}"
        break
    fi
    
    echo -n "."
    sleep 5
done
echo ""

# Verifica Finale
if [ "$RESTARTED" -eq 1 ]; then
    echo -e "${GREEN}✅ SUCCESSO: Pod riavviato automaticamente.${NC}"
    
    # Mostra lo stato finale per conferma visiva
    kubectl get pod -n kafka -l app=consumer
else
    echo -e "${RED}❌ FALLITO: Il pod è ancora bloccato dopo 100s.${NC}"
    kubectl get pod -n kafka -l app=consumer
    
    # Sblocchiamo il pod se il test fallisce, per non lasciarlo zombie
    log "Recovery: Sblocco il processo..."
    kubectl exec -n kafka $CONSUMER_POD -- /bin/sh -c "kill -CONT 1" >/dev/null 2>&1
fi
==================== FINE FILE: demoV3/07_self_healing.sh ====================

==================== FILE: demoV3/00_common.sh ====================
#!/bin/bash

# --- COLORI & STILE ---
# Definizioni ANSI per rendere l'output leggibile e professionale
GREEN='\033[0;32m'
RED='\033[0;31m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color (Reset)

# --- FUNZIONI CORE ---

# Stampa un titolo di sezione (Step) in Ciano
step() { echo -e "\n${CYAN}### $1 ###${NC}"; }

# Esegue un comando stampandolo prima in Giallo (simula verbose mode)
# Redirige l'echo del comando su stderr per non sporcare eventuali pipe
run() {
    echo -e "${YELLOW}$ $*${NC}" >&2
    "$@"
}

# Pulisce lo schermo e stampa l'intestazione principale della demo
print_header() {
    clear
    echo -e "${BLUE}================================================${NC}"
    echo -e "${BLUE}# $1${NC}"
    echo -e "${BLUE}================================================${NC}"
}

# Log informativi (Verde) e di Errore (Rosso)
log() { echo -e "${GREEN}[INFO]${NC} $1"; }
error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Mette in pausa lo script finché l'utente non preme Invio
pause() { echo -e "\n${CYAN}>>> Premi INVIO per continuare...${NC}"; read; }

# --- SETUP PERCORSI ---
# Calcola la directory assoluta dove si trova questo script per evitare errori di path relativi
COMMON_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
JWT_SCRIPT="$COMMON_DIR/../JWTtoken/gen_jwt.py"

# --- SETUP AMBIENTE ---
# Recupera IP di Minikube e Porta di Kong dinamicamente
export IP=$(minikube ip)
export PORT=$(kubectl get svc -n kong kong-kong-proxy -o jsonpath='{.spec.ports[0].nodePort}')
export BASE_URL="http://producer.$IP.nip.io:$PORT"

# --- GENERAZIONE TOKEN ---
# Verifica che lo script Python per il JWT esista
if [ ! -f "$JWT_SCRIPT" ]; then
    error "Script JWT non trovato in $JWT_SCRIPT"
    exit 1
fi

# Esegue lo script python e salva il token pulito (tr elimina a capo)
export TOKEN=$(python3 "$JWT_SCRIPT" | tr -d '\r\n')

if [ -z "$TOKEN" ]; then
    error "Errore: Token generato vuoto."
    exit 1
fi

# --- RESET DB AUTOMATICO ---
# Questa parte viene eseguita ogni volta che fai 'source 00_common.sh'.
# Garantisce che ogni demo parta da un database vuoto.
log "Reset ambiente: svuoto la collection 'events' su MongoDB..."

# Recupera la password di root di Mongo dal Secret Kubernetes
PASS=$(kubectl get secret -n kafka mongo-mongodb -o jsonpath="{.data.mongodb-root-password}" | base64 -d)

# Esegue il comando drop() tramite mongosh dentro il pod
# >/dev/null 2>&1 nasconde output tecnici non necessari (es. "Defaulted container")
kubectl exec -n kafka deployment/mongo-mongodb -- mongosh -u root -p $PASS --authenticationDatabase admin --eval 'db.getSiblingDB("student_events").events.drop()' >/dev/null 2>&1

log "Database pulito. Pronto per il test."
==================== FINE FILE: demoV3/00_common.sh ====================

==================== FILE: demoV3/04_autoscaling.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "FASE 4: HORIZONTAL POD AUTOSCALING (HPA)"

step "0. Preparazione"
log "Ripristino configurazione HPA..."
kubectl delete hpa --all -n kafka >/dev/null 2>&1
kubectl delete hpa --all -n metrics >/dev/null 2>&1

# Ripristina annotations ingress
kubectl annotate ingress producer-ingress -n kafka --overwrite \
  konghq.com/plugins="cors-plugin,jwt-auth" >/dev/null 2>&1

echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${RED}  IMPORTANTE: HPA richiede 60-120s per reagire${NC}"
echo -e "${RED}  Aprire ALTRO TERMINALE e monitorare:${NC}"
echo -e "${RED}    watch -n 2 'kubectl get hpa -n kafka'${NC}"
echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"

pause

step "1. Attivazione HPA"
log "Applico configurazione Horizontal Pod Autoscaler..."
run kubectl apply -f ../k8s/02-apps/hpa.yaml

echo -e "${YELLOW}Configurazione applicata:${NC}"
kubectl get hpa -n kafka

pause

step "2. Baseline Check"
log "Verifico stato PRIMA del carico..."
kubectl get pods -n kafka -l app=producer -o wide

pause

step "3. Stress Test CPU"
log "Genero carico CPU intensivo (4 processi paralleli)..."

echo -e "${YELLOW}━━━ STRESS TEST IN CORSO ━━━${NC}"
echo -e "${YELLOW}Guarda il terminale HPA per vedere lo scaling...${NC}"
echo -e ""

# Avvia 4 loop infiniti
for i in {1..4}; do
  (while true; do 
     curl -s -o /dev/null -X POST "$BASE_URL/event/login" \
     -H "Authorization: Bearer $TOKEN" -H "Content-Type: application/json" \
     -d '{"user_id":"stress"}' > /dev/null
     sleep 0.1  # Piccola pausa per non saturare completamente
  done) &
  echo -e "${YELLOW}Worker #$i avviato (PID: $!)${NC}"
done

echo -e "\n${CYAN}Attendi che HPA aumenti le repliche (60-90s)...${NC}"
echo -e "${RED}Quando vedi TARGETS passare da 1/4 a 2/4 o più, premi INVIO${NC}"

read

# ---
# # 1. Ferma il carico
# kill $(jobs -p) >/dev/null 2>&1
# log "Carico fermato."

# # 2. FORZATURA SCALE DOWN (La tua idea)
# step "Reset Rapido (Fast Scale-Down)"
# log "L'HPA impiegherebbe 5 minuti per scendere (stabilization window)."
# log "Lo rimuovo per forzare il ritorno a 1 replica immediatamente..."

# # Cancella l'HPA così smette di controllare il deployment
# kubectl delete hpa producer-hpa -n kafka >/dev/null 2>&1

# # Forza manualmente a 1
# run kubectl scale deploy/producer -n kafka --replicas=1

# # Aspetta che sia effettivo (opzionale, ma carino visivamente)
# log "Attendo shutdown dei pod in eccesso..."
# kubectl wait --for=delete pod -l app=producer -n kafka --timeout=60s >/dev/null 2>&1
# # Nota: wait for delete aspetta che TUTTI siano cancellati, ma siccome ne rimane 1, 
# # potrebbe andare in timeout o uscire subito. 
# # Meglio un semplice sleep o un get per far vedere che stanno morendo.
# kubectl get pods -n kafka -l app=producer

# echo -e "${GREEN}✅ Demo Autoscaling Completata.${NC}"
# --- al posto di quello sotto

# Uccidi processi stress
log "Fermo il carico..."
kill $(jobs -p) >/dev/null 2>&1

echo -e "${GREEN}✓ Stress test fermato${NC}"

step "4. Verifica Scaling"
log "Stato pods DOPO lo scaling..."
kubectl get pods -n kafka -l app=producer -o wide

echo -e "\n${CYAN}Stato HPA:${NC}"
kubectl get hpa producer-hpa -n kafka

echo -e "\n${YELLOW}Ora l'HPA inizierà lo scale-down (richiede 5min di cooldown)${NC}"
echo -e "${GREEN}✓ Autoscaling dimostrato${NC}"
==================== FINE FILE: demoV3/04_autoscaling.sh ====================

==================== FILE: demoV3/01_security.sh ====================
#!/bin/bash
source ./00_common.sh

print_header "FASE 1: SICUREZZA & SECRETS"

# --- DEFINIZIONE FUNZIONE HELPER ---
test_connect() {
  ns=$1; target_host=$2; target_port=$3; label=$4
  
  # Log su STDERR (>&2) per non sporcare la variabile di ritorno che cattura STDOUT
  log "Tentativo connessione da namespace='$ns' (pod: $label)..." >&2
  
  # Trova il nome del pod
  POD=$(kubectl get pods -n "$ns" -l "app=$label" -o jsonpath='{.items[0].metadata.name}')
  
  if [ -z "$POD" ]; then
     echo "POD_NOT_FOUND"
     return
  fi

  # Esegui test di connessione con Python (connect_ex ritorna 0 se successo, errno se errore)
  # 2>/dev/null nasconde errori di kubectl se il pod non risponde subito
  kubectl exec -n "$ns" "$POD" -- python3 -c "import socket; s=socket.socket(); s.settimeout(3); print(s.connect_ex(('$target_host', $target_port)))" 2>/dev/null
}
# ---------------------------------------------------------------------

# ==============================================================================
# 1. VERIFICA TLS
# ==============================================================================
step "1. Verifica Crittografia (TLS 1.3)"
log "Controllo che la comunicazione interna verso Kafka sia cifrata."

# Esegue openssl dentro un pod per verificare che il broker risponda in SSL
# Usa -brief per un output conciso
run kubectl exec -it -n kafka uni-it-cluster-broker-0 -- \
  openssl s_client -connect uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093 -brief </dev/null
pause

# ==============================================================================
# PARTE 2: AUTHENTICATION (Versione Vecchia - Curl Hacker)
# ==============================================================================
step "2. Test Autenticazione (JWT Enforcement)"
log "Provo a chiamare l'API senza token. Kong deve bloccarci (401)."

echo -e "${YELLOW}$ curl ... (no token)${NC}"
# Fa una richiesta senza Header Authorization e cattura solo il codice HTTP
HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "$BASE_URL/event/login" \
    -H "Content-Type: application/json" \
    -d '{"user_id":"hacker"}')

if [ "$HTTP_CODE" == "401" ]; then
    echo -e "Status: ${GREEN}401 Unauthorized${NC} (CORRETTO)"
else
    echo -e "Status: ${RED}$HTTP_CODE${NC} (ERRORE - Doveva essere 401)"
fi
pause

# ==============================================================================
# PARTE 3: SECRETS (Versione Vecchia - Python Dump)
# ==============================================================================
step "3. Verifica Protezione Credenziali"
log "Ispeziono il Deployment per confermare che le password non siano in chiaro."

# Estrae la configurazione env dal deployment e la formatta
run kubectl get deploy -n kafka producer -o json | \
python3 -c "import sys, json; print(json.dumps(json.load(sys.stdin)['spec']['template']['spec']['containers'][0]['env'], indent=2))"

echo -e "\n${GREEN}Successo: Le variabili usano 'valueFrom: secretKeyRef'.${NC}"
pause

# ==============================================================================
# PARTE 5: Verifica SASL/SCRAM Authentication
# ==============================================================================
step "5. Verifica SASL/SCRAM Authentication"
log "Controllo credenziali SCRAM per Kafka..."

# Verifica secret producer
PASS_LEN=$(kubectl get secret producer-user -n kafka -o jsonpath='{.data.password}' 2>/dev/null | base64 -d | wc -c)
if [ "$PASS_LEN" -gt 0 ]; then
    echo -e "${GREEN}✓ producer-user: Password hash presente (${PASS_LEN} caratteri)${NC}"
else
    echo -e "${RED}✗ producer-user: Secret mancante${NC}"
fi

# Verifica secret consumer
PASS_LEN=$(kubectl get secret consumer-user -n kafka -o jsonpath='{.data.password}' 2>/dev/null | base64 -d | wc -c)
if [ "$PASS_LEN" -gt 0 ]; then
    echo -e "${GREEN}✓ consumer-user: Password hash presente (${PASS_LEN} caratteri)${NC}"
else
    echo -e "${RED}✗ consumer-user: Secret mancante${NC}"
fi

echo -e "\n${CYAN}Meccanismo:${NC}"
echo -e "  • SASL: Simple Authentication and Security Layer"
echo -e "  • SCRAM-SHA-512: Salted Challenge Response Authentication Mechanism"
echo -e "  • Hash password → Protezione da rainbow table attacks"

log "Test connessione senza credenziali (opzionale, può timeout)..."
echo -e "${YELLOW}Tento connessione Kafka SENZA auth...${NC}"

timeout 5 kubectl run kafka-noauth-test --image=confluentinc/cp-kafka:latest -n kafka --rm -it --restart=Never -- \
  kafka-console-producer --broker-list uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093 --topic test 2>&1 | grep -q "Authentication failed" && \
  echo -e "${GREEN}✓ Connessione senza auth BLOCCATA${NC}" || \
  echo -e "${YELLOW}⚠ Test inconclusivo (timeout normale per security)${NC}"

echo -e "\n${GREEN}=== SECURITY LAYERS VERIFICATI ===${NC}"
echo -e "  [✓] Application: JWT Authentication"
echo -e "  [✓] Transport: TLS 1.2/1.3 Encryption"
echo -e "  [✓] Transport: SASL/SCRAM-SHA-512"
echo -e "  [✓] Network: Network Policy (configurata)"
echo -e "  [✓] Secret Management: Kubernetes Secrets"

==================== FINE FILE: demoV3/01_security.sh ====================

==================== FILE: client/index.html ====================
<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UniKafka Dashboard (Full API)</title>
    <style>
        :root { --primary: #2563eb; --success: #16a34a; --bg: #f8fafc; --panel: #ffffff; --border: #e2e8f0; }
        body { font-family: 'Segoe UI', system-ui, sans-serif; background: var(--bg); margin: 0; padding: 20px; color: #1e293b; }
        .container { max-width: 1200px; margin: 0 auto; display: grid; grid-template-columns: 320px 1fr; gap: 20px; }
        
        /* Sidebar */
        .sidebar { background: var(--panel); padding: 20px; border-radius: 12px; border: 1px solid var(--border); height: fit-content; }
        h2 { font-size: 1.1rem; margin-top: 0; border-bottom: 2px solid var(--border); padding-bottom: 10px; color: #334155; }
        .input-group { margin-bottom: 15px; }
        label { display: block; font-weight: 600; font-size: 0.85rem; margin-bottom: 5px; color: #64748b; }
        input, textarea, select { width: 100%; padding: 8px; border: 1px solid #cbd5e1; border-radius: 6px; font-family: monospace; box-sizing: border-box; }
        input:focus { outline: 2px solid var(--primary); border-color: transparent; }
        .token-area textarea { height: 60px; font-size: 0.75rem; resize: vertical; background: #eff6ff; border-color: #bfdbfe; }

        /* Main Panel */
        .main-content { display: flex; flex-direction: column; gap: 20px; }
        .card { background: var(--panel); padding: 20px; border-radius: 12px; border: 1px solid var(--border); box-shadow: 0 2px 4px rgba(0,0,0,0.02); }
        
        /* Form Grid */
        .form-grid { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-bottom: 15px; }
        
        /* Buttons */
        .btn-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(140px, 1fr)); gap: 10px; }
        button { cursor: pointer; padding: 10px; border: none; border-radius: 6px; font-weight: 600; transition: all 0.2s; display: flex; align-items: center; justify-content: center; gap: 5px; }
        .btn-primary { background: var(--primary); color: white; }
        .btn-primary:hover { background: #1d4ed8; }
        .btn-metrics { background: var(--success); color: white; }
        .btn-metrics:hover { background: #15803d; }
        .btn-magic { background: #8b5cf6; color: white; width: 100%; margin-bottom: 15px; }
        .btn-magic:hover { background: #7c3aed; }
        .btn-clear { background: white; border: 1px solid #cbd5e1; color: #475569; padding: 5px 10px; font-size: 0.8rem; }

        /* Logger */
        #logger { background: #0f172a; color: #4ade80; padding: 15px; border-radius: 8px; font-family: 'Courier New', monospace; height: 350px; overflow-y: auto; font-size: 0.85rem; white-space: pre-wrap; line-height: 1.4; }
        .log-entry { border-bottom: 1px solid #1e293b; padding-bottom: 5px; margin-bottom: 5px; }
        .log-err { color: #f87171; }
        .log-info { color: #94a3b8; }
        .log-payload { color: #fbbf24; font-size: 0.8em; }
    </style>
</head>
<body>

<div class="container">
    <div class="sidebar">
        <h2>Configurazione</h2>
        <div class="input-group">
            <label>Minikube IP</label>
            <input type="text" id="mk-ip" value="192.168.49.2" onchange="updateUrls()">
        </div>
        <div class="input-group">
            <label>Kong Port</label>
            <input type="text" id="kong-port" value="30648" onchange="updateUrls()">
        </div>
        <div class="input-group token-area">
            <label>JWT Token</label>
            <textarea id="jwt-token" placeholder="Incolla qui il token..."></textarea>
        </div>
        <hr style="border: 0; border-top: 1px solid #eee; margin: 15px 0;">
        <div class="input-group">
            <label>Base URL (Anteprima)</label>
            <input type="text" id="base-url" readonly style="background: #f1f5f9; color: #64748b; font-size: 0.8em;">
        </div>
    </div>

    <div class="main-content">
        
        <div class="card">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px;">
                <h2 style="margin:0; border:0;">Producer (Invio Eventi)</h2>
                <button class="btn-magic" style="width: auto; padding: 8px 15px;" onclick="randomizeData()">Genera Dati Random</button>
            </div>

            <div class="form-grid">
                <div class="input-group">
                    <label>User ID</label>
                    <input type="text" id="user-id" value="studente-1">
                </div>
                <div class="input-group">
                    <label>Course ID</label>
                    <input type="text" id="course-id" value="cloud-computing">
                </div>
                <div class="input-group">
                    <label>Resource ID (Quiz/Exam/File)</label>
                    <input type="text" id="resource-id" value="quiz-101">
                </div>
                <div class="input-group">
                    <label>Score (Voto)</label>
                    <input type="number" id="score" value="28" min="18" max="30">
                </div>
            </div>

            <div class="btn-grid">
                <button class="btn-primary" onclick="sendEvent('login')">Login</button>
                <button class="btn-primary" onclick="sendEvent('quiz')">Submit Quiz</button>
                <button class="btn-primary" onclick="sendEvent('download')">Download</button>
                <button class="btn-primary" onclick="sendEvent('exam')">Prenota Esame</button>
            </div>
        </div>

        <div class="card">
            <h2>Metrics (Lettura Dati)</h2>
            <div class="btn-grid">
                <button class="btn-metrics" onclick="fetchMetric('/healthz')">Health Check</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/logins')">Total Logins</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/logins/average')">Avg Logins/User</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/quiz/success-rate')">Quiz Success Rate</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/quiz/average-score')">Avg Score/Course</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/downloads')">Top Downloads</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/exams')">Exam Bookings</button>
                <button class="btn-metrics" onclick="fetchMetric('/metrics/activity/last7days')">Trend 7 Days</button>
            </div>
        </div>

        <div class="card">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                <h2 style="margin:0; border:0;">Console Log</h2>
                <button class="btn-clear" onclick="document.getElementById('logger').innerHTML = ''">Pulisci</button>
            </div>
            <div id="logger">Pronto...</div>
        </div>
    </div>
</div>

<script>
    // Dati Random
    const USERS = ["alice", "bob", "charlie", "dave", "eve", "frank", "grace"];
    const COURSES = ["cloud-computing", "cyber-security", "data-science", "iot", "web-dev"];
    const RESOURCES = ["quiz-1", "final-exam", "slide-pdf", "lab-docker", "quiz-2"];

    function updateUrls() {
        const ip = document.getElementById('mk-ip').value;
        const port = document.getElementById('kong-port').value;
        document.getElementById('base-url').value = `http://producer.${ip}.nip.io:${port}`;
    }
    
    function randomizeData() {
        document.getElementById('user-id').value = USERS[Math.floor(Math.random() * USERS.length)];
        document.getElementById('course-id').value = COURSES[Math.floor(Math.random() * COURSES.length)];
        document.getElementById('resource-id').value = RESOURCES[Math.floor(Math.random() * RESOURCES.length)];
        document.getElementById('score').value = Math.floor(Math.random() * 13) + 18; // 18-30
        log("Dati casuali generati!", "info");
    }

    function log(msg, type = 'normal') {
        const logger = document.getElementById('logger');
        const time = new Date().toLocaleTimeString();
        const cls = type === 'error' ? 'log-err' : (type === 'info' ? 'log-info' : (type === 'payload' ? 'log-payload' : ''));
        
        const entry = document.createElement('div');
        entry.className = 'log-entry ' + cls;
        entry.innerHTML = `[${time}] ${msg}`;
        logger.insertBefore(entry, logger.firstChild);
    }

    async function callApi(endpoint, method, body = null, isMetrics = false) {
        const token = document.getElementById('jwt-token').value.trim();
        if (!token) {
            log("[ERROR] Token mancante! Incollalo nella sidebar.", "error");
            return;
        }

        const ip = document.getElementById('mk-ip').value;
        const port = document.getElementById('kong-port').value;
        // Metrics e Producer usano host diversi
        const hostPrefix = isMetrics ? 'metrics' : 'producer';
        const url = `http://${hostPrefix}.${ip}.nip.io:${port}${endpoint}`;

        const headers = {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${token}`
        };

        log(`[REQ] ${method} ${endpoint}...`);
        if(body) log(`[DATA] Payload: ${JSON.stringify(body)}`, 'payload');

        try {
            const options = { method, headers };
            if (body) options.body = JSON.stringify(body);

            const res = await fetch(url, options);
            
            let data;
            const contentType = res.headers.get("content-type");
            if (contentType && contentType.indexOf("application/json") !== -1) {
                data = await res.json();
            } else {
                data = await res.text(); // Fallback per errori HTML di Kong (404/503)
            }

            if (res.ok) {
                log(`[OK] [${res.status}] Success:\n${JSON.stringify(data, null, 2)}`);
            } else {
                log(`[ERR] [${res.status}] Error:\n${typeof data === 'object' ? JSON.stringify(data, null, 2) : data}`, "error");
            }
        } catch (e) {
            log(`[NET] Network Error: ${e.message}`, "error");
        }
    }

    function sendEvent(type) {
        const user = document.getElementById('user-id').value;
        const course = document.getElementById('course-id').value;
        const resource = document.getElementById('resource-id').value;
        const score = parseInt(document.getElementById('score').value);

        let body = { user_id: user };
        let endpoint = "";

        if (type === 'login') {
            endpoint = "/event/login";
        } else if (type === 'quiz') {
            endpoint = "/event/quiz";
            body.quiz_id = resource;
            body.course_id = course;
            body.score = score;
        } else if (type === 'download') {
            endpoint = "/event/download";
            body.materiale_id = resource;
            body.course_id = course;
        } else if (type === 'exam') {
            endpoint = "/event/exam";
            body.esame_id = resource;
            body.course_id = course;
        }

        callApi(endpoint, "POST", body, false);
    }

    function fetchMetric(endpoint) {
        callApi(endpoint, "GET", null, true);
    }

    // Init
    updateUrls();
    randomizeData();
</script>

</body>
</html>
==================== FINE FILE: client/index.html ====================

==================== FILE: JWTtoken/gen_jwt.py ====================
import jwt, time

secret = "supersecret"
key = "exam-client-key"

payload = {
    "iss": key,
    "sub": "student",
    "role": "exam-client",
    "exp": int(time.time()) + 3600  # valido per 1 ora
}

token = jwt.encode(payload, secret, algorithm="HS256")
print(token)
==================== FINE FILE: JWTtoken/gen_jwt.py ====================
