Di seguito trovi il codice sorgente e la configurazione del progetto.
Usa questi file come contesto per le risposte.
==================================================


==================== FILE: src/metrics/metrics_service.py ====================
from flask import Flask, jsonify, request
from pymongo import MongoClient
from datetime import datetime, timedelta
import os, time, threading

app = Flask(__name__)

POD_NAME = os.getenv("POD_NAME", "unknown-pod")
MONGO_URI = os.environ["MONGO_URI"]

# Configurazione Mongo con Timeout breve
client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=3000)
db = client.student_events
collection = db.events

_cache = {}
_cache_lock = threading.Lock()
CACHE_TTL = int(os.getenv("METRICS_CACHE_TTL", 10))

def cache_get(key):
    with _cache_lock:
        entry = _cache.get(key)
        if not entry: return None
        value, ts = entry
        if time.time() - ts > CACHE_TTL:
            del _cache[key]
            return None
        return value

def cache_set(key, value):
    with _cache_lock:
        _cache[key] = (value, time.time())

@app.route("/healthz")
def healthz():
    mongo_ok = False
    try:
        client.admin.command('ping')
        mongo_ok = True
    except Exception as e:
        print(f"[HEALTH FAIL] Mongo unreachable: {e}", flush=True)

    status = {
        "status": "ok" if mongo_ok else "degraded", 
        "mongo_connected": mongo_ok, 
        "processed_by": POD_NAME
    }
    return jsonify(status), 200 if mongo_ok else 503


# 1. Totale logins (Cached)
@app.route("/metrics/logins", methods=["GET"])
def total_logins():
    key = "total_logins"
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    count = collection.count_documents({"type": "login"})
    resp = {"total_logins": count}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 2. Avg logins per user (Cached)
@app.route("/metrics/logins/average", methods=["GET"])
def avg_logins_per_user():
    key = "avg_logins"
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "login"}},
        {"$group": {"_id": "$user_id", "count": {"$sum": 1}}},
        {"$group": {"_id": None, "average_logins": {"$avg": "$count"}}}
    ]
    result = list(collection.aggregate(pipeline))
    resp = result[0] if result else {"average_logins": 0}
    if "_id" in resp: del resp["_id"]
    
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 3. Downloads (OTTIMIZZATO con $facet)
@app.route("/metrics/downloads", methods=["GET"])
def downloads():
    page = int(request.args.get("page", "1"))
    per_page = int(request.args.get("per_page", "50"))
    skip = (page - 1) * per_page
    key = f"downloads_p{page}_pp{per_page}"
    
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "download_materiale"}},
        {"$group": {"_id": "$materiale_id", "downloads": {"$sum": 1}}},
        {"$sort": {"downloads": -1}},
        {"$facet": {
            "metadata": [{"$count": "total"}],
            "data": [{"$skip": skip}, {"$limit": per_page}]
        }}
    ]
    
    result = list(collection.aggregate(pipeline))[0]
    total = result["metadata"][0]["total"] if result["metadata"] else 0
    items = [{"materiale_id": r["_id"], "downloads": r["downloads"]} for r in result["data"]]
    
    resp = {"data": items, "page": page, "per_page": per_page, "total": total}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 4. Exams (OTTIMIZZATO con $facet)
@app.route("/metrics/exams", methods=["GET"])
def exams():
    page = int(request.args.get("page", "1"))
    per_page = int(request.args.get("per_page", "50"))
    skip = (page - 1) * per_page
    key = f"exams_p{page}_pp{per_page}"
    
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "prenotazione_esame"}},
        {"$group": {"_id": "$course_id", "prenotazioni": {"$sum": 1}}},
        {"$sort": {"prenotazioni": -1}},
        {"$facet": {
            "metadata": [{"$count": "total"}],
            "data": [{"$skip": skip}, {"$limit": per_page}]
        }}
    ]
    
    result = list(collection.aggregate(pipeline))[0]
    total = result["metadata"][0]["total"] if result["metadata"] else 0
    items = [{"course_id": r["_id"], "prenotazioni": r["prenotazioni"]} for r in result["data"]]
    
    resp = {"data": items, "page": page, "per_page": per_page, "total": total}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 5. Quiz Success Rate
@app.route("/metrics/quiz/success-rate", methods=["GET"])
def quiz_success_rate():
    key = "quiz_success_rate"
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "quiz_submission"}},
        {"$group": {
            "_id": None,
            "total": {"$sum": 1},
            "success": {"$sum": {"$cond": [{"$gte": ["$score", 18]}, 1, 0]}}
        }},
        {"$project": {"_id": 0, "success_rate": {"$cond": [{"$eq": ["$total", 0]}, 0, {"$multiply": [{"$divide": ["$success", "$total"]}, 100]}]}}}
    ]
    result = list(collection.aggregate(pipeline))
    resp = result[0] if result else {"success_rate": 0}
    
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

# 6. Activity Trend (LIVE - Usa _ingest_ts)
@app.route("/metrics/activity/last7days", methods=["GET"])
def activity_trend():
    # Usa _ingest_ts aggiunto dal Consumer FIXATO
    since = datetime.utcnow() - timedelta(days=7)
    pipeline = [
        {"$match": {"_ingest_ts": {"$gte": since}}},
        {"$group": {"_id": {"$dateToString": {"format": "%Y-%m-%d", "date": "$_ingest_ts"}}, "count": {"$sum": 1}}},
        {"$sort": {"_id": 1}}
    ]
    result = list(collection.aggregate(pipeline))
    return jsonify({"data": result, "source": "live", "processed_by": POD_NAME})

# 7. Avg Score Course (OTTIMIZZATO con $facet)
@app.route("/metrics/quiz/average-score", methods=["GET"])
def avg_score_per_course():
    page = int(request.args.get("page", "1"))
    per_page = int(request.args.get("per_page", "50"))
    skip = (page - 1) * per_page
    key = f"avg_score_p{page}_pp{per_page}"
    
    cached = cache_get(key)
    if cached: return jsonify({**cached, "source": "cache", "processed_by": POD_NAME})

    pipeline = [
        {"$match": {"type": "quiz_submission"}},
        {"$group": {"_id": "$course_id", "average_score": {"$avg": "$score"}}},
        {"$sort": {"average_score": -1}},
        {"$facet": {
            "metadata": [{"$count": "total"}],
            "data": [{"$skip": skip}, {"$limit": per_page}]
        }}
    ]
    
    result = list(collection.aggregate(pipeline))[0]
    total = result["metadata"][0]["total"] if result["metadata"] else 0
    items = [{"course_id": r["_id"], "average_score": r["average_score"]} for r in result["data"]]
    
    resp = {"data": items, "page": page, "per_page": per_page, "total": total}
    cache_set(key, resp)
    return jsonify({**resp, "source": "db", "processed_by": POD_NAME})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)
==================== FINE FILE: src/metrics/metrics_service.py ====================

==================== FILE: src/metrics/Dockerfile ====================
FROM python:3.11-slim

WORKDIR /app

COPY metrics_service.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "metrics_service.py"]

==================== FINE FILE: src/metrics/Dockerfile ====================

==================== FILE: src/metrics/requirements.txt ====================
flask
pymongo
==================== FINE FILE: src/metrics/requirements.txt ====================

==================== FILE: src/producer/Dockerfile ====================
FROM python:3.11-slim

WORKDIR /app
COPY producer.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 5000

CMD ["python", "producer.py"]

==================== FINE FILE: src/producer/Dockerfile ====================

==================== FILE: src/producer/requirements.txt ====================
kafka-python
flask
==================== FINE FILE: src/producer/requirements.txt ====================

==================== FILE: src/producer/producer.py ====================
from flask import Flask, request, jsonify
from kafka import KafkaProducer
from kafka.errors import NoBrokersAvailable, KafkaError
import json, os, uuid, time, threading, queue

app = Flask(__name__)

# --- Configurazione (Variabili d'ambiente e default) ---
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
# Percorso del certificato CA per la connessione sicura a Kafka
KAFKA_CA = os.getenv("KAFKA_CA", "/etc/ssl/certs/kafka/ca.crt")
# Nome del pod corrente, utile per il debug e per vedere il load balancing
POD_NAME = os.getenv("POD_NAME", "unknown-pod")
TOPIC = os.getenv("KAFKA_TOPIC", "student-events")

# Configurazione della coda locale (Fallback)
# Se Kafka è giù, accumula fino a 500 messaggi in RAM prima di rifiutare (Backpressure)
MAX_LOCAL_QUEUE = int(os.getenv("MAX_LOCAL_QUEUE", 500))   # max events to hold locally
# Intervallo in secondi per il thread che prova a svuotare la coda locale verso Kafka
QUEUE_FLUSH_INTERVAL = float(os.getenv("QUEUE_FLUSH_INTERVAL", 3.0))  # 3 secondi

# Coda thread-safe per il buffering locale
local_queue = queue.Queue(maxsize=MAX_LOCAL_QUEUE)
producer = None
# Lock per garantire l'accesso thread-safe all'oggetto producer
producer_lock = threading.Lock()
shutdown_flag = threading.Event()

# Contatori per le metriche interne (monitoraggio stato)
sent_counter = 0
queued_counter = 0
failed_sends = 0
counters_lock = threading.Lock()

# --- Inizializzazione Producer (Tentativo non bloccante) ---
def init_producer():
    global producer
    try:
        # Configurazione del client Kafka con SASL/SCRAM e SSL
        p = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP,
            security_protocol="SASL_SSL" if SASL_USERNAME else "PLAINTEXT",
            sasl_mechanism="SCRAM-SHA-512" if SASL_USERNAME else None,
            sasl_plain_username=SASL_USERNAME if SASL_USERNAME else None,
            sasl_plain_password=SASL_PASSWORD if SASL_USERNAME else None,
            ssl_cafile=KAFKA_CA if SASL_USERNAME else None,
            value_serializer=lambda v: json.dumps(v).encode("utf-8"),
            acks='all',          # Richiede conferma da tutte le repliche (Durabilità)
            enable_idempotence=True,
            retries=5,           # Riprova in caso di errori di rete temporanei
            linger_ms=10,        # Attende 10ms per raggruppare i messaggi (Batching)
            batch_size=16384,
            max_block_ms=1000    # Blocca al massimo 1s se il buffer locale di Kafka è pieno
        )
        with producer_lock:
            producer = p
        print(f"[PRODUCER {POD_NAME}] KafkaProducer initialized.", flush=True)
    except Exception as e:
        with producer_lock:
            producer = None
        print(f"[PRODUCER {POD_NAME}] Kafka init failed: {e}", flush=True)

# Tentativo di connessione all'avvio dell'applicazione
init_producer()

# --- Callback Asincrone ---
def on_send_success(record_metadata):
    global sent_counter
    with counters_lock:
        sent_counter += 1

def on_send_error(ex):
    global failed_sends
    with counters_lock:
        failed_sends += 1
    print(f"[PRODUCER {POD_NAME}] Kafka send error: {ex}", flush=True)

# # --- Thread di Background (Gestione Coda Locale) ---
# def flush_loop():
#     """
#     Thread che gira in background per svuotare la 'local_queue'.
#     Gestisce il reinvio dei messaggi accumulati quando Kafka era irraggiungibile.
#     """
#     global producer, queued_counter
#     while not shutdown_flag.is_set():
#         # Assicura che il producer sia inizializzato
#         with producer_lock:
#             if producer is None:
#                 init_producer()
        
#         # Prova a svuotare la coda messaggio per messaggio
#         while local_queue:
#             event = local_queue[0]  # Legge il primo elemento senza rimuoverlo (Peek)
#             try:
#                 with producer_lock:
#                     if producer is None:
#                         raise NoBrokersAvailable("producer non pronto")
#                     # Invio asincrono
#                     future = producer.send(TOPIC, value=event)
#                     future.add_callback(on_send_success)
#                     future.add_errback(on_send_error)
                
#                 # Se send() non ha dato errori immediati, rimuoviamo dalla coda locale
#                 local_queue.popleft()
#                 queued_counter = max(0, queued_counter - 1)
#             except NoBrokersAvailable:
#                 # Kafka ancora giù, interrompiamo il ciclo e riproviamo dopo lo sleep
#                 break
#             except KafkaError as e:
#                 # Errore specifico di Kafka, logghiamo e riproviamo
#                 print(f"[PRODUCER {POD_NAME}] Errore Kafka durante flush coda: {e}", flush=True)
#                 time.sleep(0.1)
        
#         # Forza l'invio dei messaggi nel buffer del client Kafka
#         try:
#             with producer_lock:
#                 if producer:
#                     producer.flush(timeout=0.1)
#         except Exception:
#             pass

#         # Attesa prima del prossimo ciclo di controllo
#         time.sleep(QUEUE_FLUSH_INTERVAL)

# --- Thread di Background (Gestione Coda Locale) ---
def flush_loop():
    global producer
    while not shutdown_flag.is_set():
        # Assicura che il producer sia inizializzato
        with producer_lock:
            p = producer
        if p is None:
            init_producer()
            with producer_lock:
                p = producer

        # Prova a svuotare la coda messaggio per messaggio
        while True:
            try:
                event = local_queue.get_nowait()
            except queue.Empty:
                break

            try:
                with producer_lock:
                    if producer is None:
                        raise NoBrokersAvailable("producer not ready")
                    # Invio asincrono
                    future = producer.send(TOPIC, value=event)
                    future.add_callback(on_send_success)
                    future.add_errback(on_send_error)
                
                with counters_lock:
                    queued_counter = max(0, queued_counter - 1)
                
                local_queue.task_done()
            except NoBrokersAvailable:
                # Kafka ancora giù, interrompiamo il ciclo e riproviamo dopo lo sleep
                try:
                    local_queue.put_nowait(event)
                except queue.Full:
                    # Impossibile rimettere in coda, perdiamo l'evento
                    with counters_lock:
                        failed_sends += 1
                    print(f"[PRODUCER {POD_NAME}] CRITICAL: Lost event during flush, queue full while requeueing", flush=True)
                break
            except KafkaError as e:
                print(f"[PRODUCER {POD_NAME}] KafkaError while flushing queue: {e}", flush=True)
                time.sleep(0.1)

        # Forza l'invio dei messaggi nel buffer del client Kafka
        try:
            with producer_lock:
                if producer:
                    producer.flush(timeout=0.1)
        except Exception:
            pass

        time.sleep(QUEUE_FLUSH_INTERVAL)

# Avvio del thread di flush
flush_thread = threading.Thread(target=flush_loop, daemon=True)
flush_thread.start()

# --- Funzione Core di Invio (Logica Ibrida) ---
def try_send_event(event):
    """
    Try to send via KafkaProducer asynchronously.
    If Kafka is not available, enqueue locally (bounded queue).
    Returns tuple (status, message).
    """
    global producer, queued_counter

    with producer_lock:
        p = producer

    # 1. Prova invio diretto a Kafka
    if p:
        try:
            future = p.send(TOPIC, value=event)
            future.add_callback(on_send_success)
            future.add_errback(on_send_error)
            return ("sent", "event queued to kafka client buffer")
        except (NoBrokersAvailable, KafkaError) as e:
            print(f"[PRODUCER {POD_NAME}] Send error, will enqueue locally: {e}", flush=True)

    # 2. FALLBACK: Coda Locale in RAM
    try:
        local_queue.put_nowait(event)
        with counters_lock:
            global queued_counter
            queued_counter += 1
        return ("queued", f"event enqueued locally ({local_queue.qsize()}/{MAX_LOCAL_QUEUE})")
    except queue.Full:
        with counters_lock:
            global failed_sends
            failed_sends += 1
        # 3. BACKPRESSURE: Coda piena, rifiutiamo la richiesta
        return ("rejected", "Backpressure active: local queue full, event rejected")

# --- Endpoints ---
@app.route("/healthz")
def healthz():
    """
    Endpoint per le Probes di Kubernetes.
    Ritorna 503 se Kafka non è connesso, così il pod viene rimosso dal Load Balancer.
    """
    with producer_lock:
        p = producer
    with counters_lock:
        sc = sent_counter
        qc = queued_counter
        fs = failed_sends

    status = {
        "status": "ok" if p else "degraded",
        "producer_connected": bool(p),
        "local_queue_len": local_queue.qsize(),
        "queue_capacity": MAX_LOCAL_QUEUE,
        "sent_counter": sc,
        "queued_counter": qc,
        "failed_sends": fs,
        "processed_by": POD_NAME
    }
    return jsonify(status), 200 if p else 503

@app.route("/event/login", methods=["POST"])
def produce_login():
    data = request.json or {}
    required = ["user_id"]
    if not all(k in data for k in required):
        return jsonify({"error": "Missing required field: user_id", "processed_by": POD_NAME}), 400

    eid = data.get("event_id") or str(uuid.uuid4())
    event = {
        "event_id": eid,
        "type": "login",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        # Qui scatta il Backpressure (503 Service Unavailable)
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503

@app.route("/event/quiz", methods=["POST"])
def produce_quiz():
    data = request.json or {}
    required = ["user_id", "quiz_id", "course_id", "score"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing required fields: {required}", "processed_by": POD_NAME}), 400

    event = {
        "event_id": str(uuid.uuid4()),
        "type": "quiz_submission",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"],
        "quiz_id": data["quiz_id"],
        "course_id": data["course_id"],
        "score": data["score"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503
    
@app.route("/event/download", methods=["POST"])
def produce_download():
    data = request.json or {}
    required = ["user_id", "materiale_id", "course_id"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing required fields: {required}", "processed_by": POD_NAME}), 400

    event = {
        "event_id": str(uuid.uuid4()),
        "type": "download_materiale",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"],
        "materiale_id": data["materiale_id"],
        "course_id": data["course_id"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503

@app.route("/event/exam", methods=["POST"])
def produce_exam():
    data = request.json or {}
    required = ["user_id", "esame_id", "course_id"]
    if not all(k in data for k in required):
        return jsonify({"error": f"Missing required fields: {required}", "processed_by": POD_NAME}), 400

    event = {
        "event_id": str(uuid.uuid4()),
        "type": "prenotazione_esame",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "user_id": data["user_id"],
        "esame_id": data["esame_id"],
        "course_id": data["course_id"]
    }

    status, msg = try_send_event(event)
    if status == "sent":
        return jsonify({"status": "ok", "event": event, "info": msg, "processed_by": POD_NAME}), 200
    elif status == "queued":
        return jsonify({"status": "accepted", "event": event, "info": msg, "processed_by": POD_NAME}), 202
    else:
        return jsonify({"status": "error", "error": msg, "processed_by": POD_NAME}), 503

@app.route("/metrics", methods=["GET"])
def metrics():
    with counters_lock:
        sc = sent_counter
        qc = queued_counter
        fs = failed_sends
    return jsonify({
        "sent_counter": sc,
        "queued_local": local_queue.qsize(),
        "queue_capacity": MAX_LOCAL_QUEUE,
        "queued_counter": qc,
        "failed_sends": fs,
        "processed_by": POD_NAME
    }), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
==================== FINE FILE: src/producer/producer.py ====================

==================== FILE: src/consumer/Dockerfile ====================
FROM python:3.11-slim

WORKDIR /app
COPY consumer.py requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "consumer.py"]

==================== FINE FILE: src/consumer/Dockerfile ====================

==================== FILE: src/consumer/consumer.py ====================
# consumer.py (bulk + idempotency + flush timeout + heartbeat + _ingest_ts)
from kafka import KafkaConsumer
from kafka.errors import NoBrokersAvailable
from pymongo import MongoClient, errors
from pymongo.errors import ServerSelectionTimeoutError
from datetime import datetime, timedelta, timezone
import json, os, time, sys, threading, pathlib

# --- Configurazione Variabili d'Ambiente ---
KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP")
SASL_USERNAME = os.getenv("SASL_USERNAME")
SASL_PASSWORD = os.getenv("SASL_PASSWORD")
KAFKA_CA = os.getenv("KAFKA_CA", "/etc/ssl/certs/kafka/ca.crt")
TOPIC = os.getenv("KAFKA_TOPIC", "student-events")
MONGO_URI = os.environ["MONGO_URI"]

# Configurazione del Batching (Bulk Processing)
BUFFER_SIZE = int(os.getenv("CONSUMER_BUFFER_SIZE", "20"))   # flush ogni N eventi
FLUSH_TIMEOUT = int(os.getenv("CONSUMER_FLUSH_TIMEOUT", "5"))  # o flush ogni X secondi

# File di Heartbeat per la Liveness Probe di Kubernetes
HEARTBEAT_FILE = "/tmp/heartbeat"

# --- Connessione Mongo (Retry Loop) ---
def get_mongo_collection():
    retries = 0
    while True:
        try:
            print(f"[CONSUMER] Tentativo connessione MongoDB... ({retries+1})", flush=True)
            client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            client.admin.command('ping')
            print("[CONSUMER] Connesso a MongoDB!", flush=True)
            return client.student_events.events
        except (ServerSelectionTimeoutError, Exception) as e:
            retries += 1
            print(f"[CONSUMER] MongoDB non pronto ({e}). Attendo 5 secondi...", flush=True)
            time.sleep(5)

# --- Connessione Kafka (Retry Loop) ---
def get_kafka_consumer():
    retries = 0
    while True:
        try:
            print(f"[CONSUMER] Tentativo connessione Kafka... ({retries+1})", flush=True)
            consumer = KafkaConsumer(
                TOPIC,
                bootstrap_servers=KAFKA_BOOTSTRAP,
                security_protocol="SASL_SSL" if SASL_USERNAME else "PLAINTEXT",
                sasl_mechanism="SCRAM-SHA-512" if SASL_USERNAME else None,
                sasl_plain_username=SASL_USERNAME if SASL_USERNAME else None,
                sasl_plain_password=SASL_PASSWORD if SASL_USERNAME else None,
                ssl_cafile=KAFKA_CA if SASL_USERNAME else None,
                auto_offset_reset='earliest',
                group_id='db-consumer-group',
                enable_auto_commit=False,  # relativo a at-least-once
                value_deserializer=lambda v: json.loads(v.decode('utf-8'))
            )
            print("[CONSUMER] Consumer Kafka pronto e sottoscritto!", flush=True)
            return consumer
        except NoBrokersAvailable:
            retries += 1
            print(f"[CONSUMER] Kafka non pronto. Attendo 5 secondi...", flush=True)
            time.sleep(5)
        except Exception as e:
            print(f"[CONSUMER] Errore Kafka generico: {e}. Riprovo tra 5s...", flush=True)
            time.sleep(5)

# --- Setup ---
collection = get_mongo_collection()
consumer = get_kafka_consumer()

# heartbeat loop (per liveness probe)
def heartbeat_loop():
    p = pathlib.Path(HEARTBEAT_FILE)
    while True:
        try:
            # scriviamo semplicemente qualcosa per aggiornare il mtime
            p.write_bytes(b"ok")
        except Exception as e:
            print(f"[HEARTBEAT] write failed: {e}", flush=True)
        time.sleep(5)

hb_thread = threading.Thread(target=heartbeat_loop, daemon=True)
hb_thread.start()

# Buffer locale per il Bulk Insert
buffer = []
last_flush = datetime.utcnow()

def log_event(event):
    t = event.get("type")
    u = event.get("user_id")
    eid = event.get("event_id")
    print(f"[PROCESSED] event_id={eid} Type: {t}, User: {u}", flush=True)

def flush_buffer_and_commit():
    """
    Tenta il bulk insert e, se avvenuto con successo, esegue commit.
    """
    global buffer, last_flush
    if not buffer:
        return True

    # Prepara i documenti
    docs = []
    for ev in buffer:
        eid = ev.get("event_id") or ev.get("eventId") or None
        if not eid:
            eid = str(datetime.utcnow().timestamp()) + "-" + (ev.get("user_id","unknown"))
            ev["event_id"] = eid
        # Aggiungi ingest timestamp (utile per metriche)
        ev["_ingest_ts"] = datetime.utcnow().replace(tzinfo=timezone.utc)
        doc = {"_id": eid, **ev}
        docs.append(doc)

    try:
        # Scrittura su DB
        collection.insert_many(docs, ordered=False)

        print(f"[FLUSH] Inseriti {len(docs)} documenti su MongoDB:", flush=True)
        for d in docs:
            print(f"   -> Processed: {d.get('user_id')}", flush=True)

        buffer = []
        last_flush = datetime.utcnow()
        consumer.commit()
        return True

    except errors.BulkWriteError as bwe:
        # Gestione Idempotenza: Se ci sono duplicati, Mongo lancia questo errore.
        # i nuovi sono scritti, i doppi scartati
        inserted = bwe.details.get('nInserted') if bwe.details else None
        print(f"[WARN] BulkWriteError (possibili duplicati): inserted={inserted}", flush=True)
        for d in docs:
            print(f"   -> Processed (o duplicato): {d.get('user_id')}", flush=True)

        buffer = []
        last_flush = datetime.utcnow()
        consumer.commit()
        return True

    except Exception as e:
        print(f"[ERROR] Errore insert_many: {e}", flush=True)
        return False

# --- Ciclo Principale di Consumo ---
consumer.subscribe([TOPIC])

print(f"[CONSUMER] Iniziato loop di consumo (Polling)...", flush=True)

try:
    while True:
        # 1. POLL: Legge messaggi per 1 secondo (1000ms)
        msg_pack = consumer.poll(timeout_ms=1000)

        for topic_partition, messages in msg_pack.items():
            for message in messages:
                buffer.append(message.value)

        # 2. CONTROLLI DI FLUSH (Eseguiti anche se non arrivano messaggi)
        now = datetime.utcnow()

        buffer_full = len(buffer) >= BUFFER_SIZE
        timeout_reached = (now - last_flush).total_seconds() >= FLUSH_TIMEOUT

        if buffer and (buffer_full or timeout_reached):
            ok = flush_buffer_and_commit()
            if not ok:
                print("[CONSUMER] Flush fallito, riprovo al prossimo giro...", flush=True)
                time.sleep(5)

except KeyboardInterrupt:
    print("Shutdown by user", flush=True)
    # Tenta un ultimo flush prima di chiudere
    flush_buffer_and_commit()
    sys.exit(0)
except Exception as e:
    print(f"[FATAL] {e}", flush=True)
    sys.exit(1)

==================== FINE FILE: src/consumer/consumer.py ====================

==================== FILE: src/consumer/requirements.txt ====================
kafka-python
pymongo
==================== FINE FILE: src/consumer/requirements.txt ====================

==================== FILE: k8s/00-infrastructure/kafka-cluster.yaml ====================
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: controller
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  replicas: 2
  roles:
    - controller
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 5Gi
        kraftMetadata: shared
        deleteClaim: false
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: broker
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  replicas: 2
  roles:
    - broker
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 5Gi
        #Per i broker, lo storage deve essere SOLO per i log dei dati, non per il metadata quorum. 
        #kraftMetadata: shared
        deleteClaim: false
---
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: uni-it-cluster
  namespace: kafka
spec:
  kafka:
    version: 4.1.0
    metadataVersion: 4.1-IV1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
        authentication:
          type: scram-sha-512
    config:
      #Aumenta il fattore di replicazione a 3 per una maggiore tolleranza ai guasti
      offsets.topic.replication.factor: 2
      transaction.state.log.replication.factor: 2
      default.replication.factor: 2
      transaction.state.log.min.isr: 1 
      min.insync.replicas: 1
  entityOperator:
    topicOperator: {}
    userOperator: {}

==================== FINE FILE: k8s/00-infrastructure/kafka-cluster.yaml ====================

==================== FILE: k8s/00-infrastructure/kafka-topic.yaml ====================
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: student-events
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  partitions: 3
  replicas: 2  # 2 --> se un broker muore, l'altro ha i dati
  config:
    retention.ms: 604800000
    segment.bytes: 1073741824
    min.insync.replicas: 1 
==================== FINE FILE: k8s/00-infrastructure/kafka-topic.yaml ====================

==================== FILE: k8s/00-infrastructure/users.yaml ====================
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: producer-user
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  authentication:
    type: scram-sha-512
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaUser
metadata:
  name: consumer-user
  namespace: kafka
  labels:
    strimzi.io/cluster: uni-it-cluster
spec:
  authentication:
    type: scram-sha-512
==================== FINE FILE: k8s/00-infrastructure/users.yaml ====================

==================== FILE: k8s/03-gateway/jwt-plugin-metrics.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
  namespace: metrics
plugin: jwt
config:
  claims_to_verify:
    - exp
==================== FINE FILE: k8s/03-gateway/jwt-plugin-metrics.yaml ====================

==================== FILE: k8s/03-gateway/jwt-consumer.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongConsumer
metadata:
  name: exam-client
  namespace: kafka
  annotations:
    kubernetes.io/ingress.class: kong
username: exam-client
credentials:
  - exam-client-jwt
==================== FINE FILE: k8s/03-gateway/jwt-consumer.yaml ====================

==================== FILE: k8s/03-gateway/rate-limit-plugin.yaml ====================
# Policy per il namespace KAFKA (Producer)
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limit
  namespace: kafka
plugin: rate-limiting
config:
  second: 3        # 3 richieste al secondo
  policy: local
  hide_client_headers: false # true per vedere gli header di debug
---
# Policy per il namespace METRICS (Metrics Service)
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: rate-limit
  namespace: metrics
plugin: rate-limiting
config:
  second: 20       # 20 richieste al secondo
  policy: local
  hide_client_headers: false
==================== FINE FILE: k8s/03-gateway/rate-limit-plugin.yaml ====================

==================== FILE: k8s/03-gateway/jwt-plugin-kafka.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongPlugin
metadata:
  name: jwt-auth
  namespace: kafka
plugin: jwt
config:
  claims_to_verify:
    - exp
==================== FINE FILE: k8s/03-gateway/jwt-plugin-kafka.yaml ====================

==================== FILE: k8s/03-gateway/kong-cors-plugin.yaml ====================
apiVersion: configuration.konghq.com/v1
kind: KongClusterPlugin
metadata:
  name: cors-plugin
  annotations:
    kubernetes.io/ingress.class: kong
  labels:
    global: "true"
plugin: cors
config:
  origins:
    - "*"
  methods:
    - GET
    - POST
    - OPTIONS
  headers:
    - Authorization
    - Content-Type
  max_age: 3600
==================== FINE FILE: k8s/03-gateway/kong-cors-plugin.yaml ====================

==================== FILE: k8s/03-gateway/metrics-ingress.yaml ====================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: metrics-ingress
  namespace: metrics
  annotations:
    konghq.com/plugins: jwt-auth, rate-limit, cors-plugin
    konghq.com/strip-path: "false"
    kubernetes.io/ingress.class: kong
spec:
  ingressClassName: kong
  rules:
    - host: metrics.192.168.49.2.nip.io
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: metrics-service
                port:
                  number: 5001
==================== FINE FILE: k8s/03-gateway/metrics-ingress.yaml ====================

==================== FILE: k8s/03-gateway/producer-ingress.yaml ====================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: producer-ingress
  namespace: kafka
  annotations:
    konghq.com/plugins: jwt-auth, rate-limit, cors-plugin
    kubernetes.io/ingress.class: kong
spec:
  ingressClassName: kong
  rules:
  - host: producer.192.168.49.2.nip.io
    http:
      paths:
        - path: /event
          pathType: Prefix
          backend:
            service:
              name: producer-service
              port:
                number: 5000
==================== FINE FILE: k8s/03-gateway/producer-ingress.yaml ====================

==================== FILE: k8s/01-security/mongo-network-policy.yaml ====================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-mongo-access
  namespace: kafka
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: mongodb # Label standard del chart Bitnami
  policyTypes:
  - Ingress
  ingress:
  # Regola 1: Permette accesso dal Consumer (stesso namespace 'kafka')
  - from:
    - podSelector:
        matchLabels:
          app: consumer
    ports:
    - protocol: TCP
      port: 27017

  # Regola 2: Permette accesso dal Metrics Service (namespace 'metrics')
  - from:
    - namespaceSelector:
        matchLabels:
          name: metrics   # Cerca il namespace con label name=metrics
      podSelector:
        matchLabels:
          app: metrics-service
    ports:
    - protocol: TCP
      port: 27017
==================== FINE FILE: k8s/01-security/mongo-network-policy.yaml ====================

==================== FILE: k8s/01-security/jwt-credential.yaml ====================
apiVersion: v1
kind: Secret
metadata:
  name: exam-client-jwt
  namespace: kafka
  labels:
    konghq.com/credential: jwt
type: Opaque
stringData:
  key: "exam-client-key"
  algorithm: "HS256"
  secret: "supersecret"
==================== FINE FILE: k8s/01-security/jwt-credential.yaml ====================

==================== FILE: k8s/02-apps/consumer-deployment.yaml ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: consumer
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: consumer
  template:
    metadata:
      labels:
        app: consumer
    spec:
      containers:
      - name: consumer
        image: consumer:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: KAFKA_BOOTSTRAP
          value: "uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093"
        - name: SASL_USERNAME
          value: "consumer-user"   
        - name: SASL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: consumer-user
              key: password         
        - name: KAFKA_CA
          value: "/etc/ssl/certs/kafka/ca.crt"
        - name: MONGO_URI
          valueFrom:
            secretKeyRef:
              name: mongo-creds
              key: MONGO_URI
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - test -f /tmp/heartbeat && [ $(($(date +%s) - $(date -r /tmp/heartbeat +%s))) -lt 30 ]
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        volumeMounts:
        - name: kafka-ca
          mountPath: /etc/ssl/certs/kafka
          readOnly: true
      volumes:
      - name: kafka-ca
        secret:
          secretName: kafka-ca-cert

==================== FINE FILE: k8s/02-apps/consumer-deployment.yaml ====================

==================== FILE: k8s/02-apps/producer-deployment.yaml ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: producer
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: producer
  template:
    metadata:
      labels:
        app: producer
    spec:
      containers:
      - name: producer
        image: producer:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: KAFKA_BOOTSTRAP
          value: "uni-it-cluster-kafka-bootstrap.kafka.svc.cluster.local:9093"
        - name: SASL_USERNAME
          value: "producer-user"  
        - name: SASL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: producer-user
              key: password         
        - name: KAFKA_CA
          value: "/etc/ssl/certs/kafka/ca.crt"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
        ports:
        - containerPort: 5000
        #probes
        readinessProbe:
          httpGet:
            path: /healthz
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /healthz
            port: 5000
          initialDelaySeconds: 15
          periodSeconds: 10
          failureThreshold: 6   # Sii paziente prima di ucciderlo (potrebbe essere solo lento)
        volumeMounts:
        - name: kafka-ca
          mountPath: /etc/ssl/certs/kafka
          readOnly: true
      volumes:
      - name: kafka-ca
        secret:
          secretName: kafka-ca-cert
---
apiVersion: v1
kind: Service
metadata:
  name: producer-service
  namespace: kafka
spec:
  selector:
    app: producer
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
  type: ClusterIP

==================== FINE FILE: k8s/02-apps/producer-deployment.yaml ====================

==================== FILE: k8s/02-apps/hpa.yaml ====================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: producer-hpa
  namespace: kafka
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: producer
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50   # scala sopra il 50% di CPU media
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: consumer-hpa
  namespace: kafka
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: consumer
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: metrics-service-hpa
  namespace: metrics
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: metrics-service
  minReplicas: 1
  maxReplicas: 4
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

==================== FINE FILE: k8s/02-apps/hpa.yaml ====================

==================== FILE: k8s/02-apps/metrics-deployment.yaml ====================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-service
  namespace: metrics
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics-service
  template:
    metadata:
      labels:
        app: metrics-service
    spec:
      containers:
      - name: metrics-service
        image: metrics-service:latest
        imagePullPolicy: IfNotPresent
        env:
        - name: MONGO_URI
          valueFrom:
            secretKeyRef:
              name: mongo-creds
              key: MONGO_URI
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        ports:
        - containerPort: 5001
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "256Mi"
        #probes
        readinessProbe:
          httpGet:
            path: /healthz
            port: 5001
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /healthz
            port: 5001
          initialDelaySeconds: 15
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-service
  namespace: metrics
spec:
  selector:
    app: metrics-service
  ports:
    - protocol: TCP
      port: 5001
      targetPort: 5001
  type: ClusterIP

==================== FINE FILE: k8s/02-apps/metrics-deployment.yaml ====================

==================== FILE: JWTtoken/gen_jwt.py ====================
import jwt, time

secret = "supersecret"
key = "exam-client-key"

payload = {
    "iss": key,
    "sub": "student",
    "role": "exam-client",
    "exp": int(time.time()) + 3600  # valido per 1 ora
}

token = jwt.encode(payload, secret, algorithm="HS256")
print(token)
==================== FINE FILE: JWTtoken/gen_jwt.py ====================
